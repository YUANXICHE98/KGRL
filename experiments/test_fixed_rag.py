"""
æµ‹è¯•ä¿®å¤çš„RAG Agent
å¯¹æ¯” Baseline vs Fixed RAG vs Original RAG
"""

import sys
import time
import json
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from src.agents.baseline_agent import BaselineAgent
from src.agents.rag_agent import RAGAgent
from src.agents.fixed_rag_agent import FixedRAGAgent
from src.environments.textworld_env import TextWorldEnvironment
from src.knowledge.kg_builder import KnowledgeGraphBuilder
from src.knowledge.kg_retriever import KnowledgeGraphRetriever
from src.utils.logger import get_logger

def create_enhanced_knowledge_graph():
    """åˆ›å»ºå¢å¼ºçš„çŸ¥è¯†å›¾è°±ï¼ŒåŒ…å«å¯¼èˆªä¿¡æ¯"""
    kg = KnowledgeGraphBuilder('enhanced_game_knowledge')
    
    # æ ¸å¿ƒä»»åŠ¡çŸ¥è¯†
    kg.add_fact('goal', 'is', 'find key and open chest')
    kg.add_fact('key', 'location', 'kitchen')
    kg.add_fact('chest', 'location', 'bedroom')
    kg.add_fact('key', 'opens', 'chest')
    
    # å¯¼èˆªçŸ¥è¯†ï¼ˆå…³é”®ï¼ï¼‰
    kg.add_fact('kitchen', 'exit', 'north')
    kg.add_fact('go north', 'leads to', 'living room')
    kg.add_fact('living room', 'exit east', 'bedroom')
    kg.add_fact('go east', 'leads to', 'bedroom')
    kg.add_fact('bedroom', 'contains', 'chest')
    
    # åŠ¨ä½œçŸ¥è¯†
    kg.add_fact('take', 'works with', 'key')
    kg.add_fact('open', 'works with', 'chest')
    kg.add_fact('after taking key', 'next step', 'go to bedroom')
    kg.add_fact('path to bedroom', 'is', 'north then east')
    
    # ç­–ç•¥çŸ¥è¯†
    kg.add_fact('strategy step 1', 'take', 'key')
    kg.add_fact('strategy step 2', 'go', 'north')
    kg.add_fact('strategy step 3', 'go', 'east')
    kg.add_fact('strategy step 4', 'open', 'chest')
    
    return kg

def run_single_episode_debug(agent, env, agent_name, episode_num, max_steps=15):
    """è¿è¡Œå•ä¸ªepisodeå¹¶æ‰“å°è¯¦ç»†è°ƒè¯•ä¿¡æ¯"""
    logger = get_logger(f"{agent_name}_Debug")
    
    print(f"\nğŸ”¬ {agent_name} Episode {episode_num} å¼€å§‹")
    print("-" * 50)
    
    # é‡ç½®ç¯å¢ƒå’ŒAgent
    observation = env.reset()
    if hasattr(agent, 'reset'):
        agent.reset()
    
    total_reward = 0
    done = False
    step_count = 0
    actions_taken = []
    
    print(f"åˆå§‹è§‚å¯Ÿ: {observation}")
    print(f"ç›®æ ‡: Find the key and open the chest in the bedroom")
    
    while not done and step_count < max_steps:
        available_actions = env.get_available_actions()
        print(f"\n--- Step {step_count + 1} ---")
        print(f"å¯ç”¨åŠ¨ä½œ: {available_actions}")
        
        # Agentå†³ç­–
        start_time = time.time()
        try:
            action = agent.act(observation, available_actions)
            api_time = time.time() - start_time
            
            print(f"Agenté€‰æ‹©: '{action}' ({api_time:.2f}s)")
            print(f"åŠ¨ä½œæœ‰æ•ˆ: {action in available_actions}")
            
            # æ˜¾ç¤ºRAG Agentçš„çŸ¥è¯†ä½¿ç”¨æƒ…å†µ
            if hasattr(agent, 'get_stats'):
                stats = agent.get_stats()
                if stats.get('kg_queries', 0) > 0:
                    print(f"KGä½¿ç”¨: {stats['kg_queries']}æŸ¥è¯¢, {stats['kg_hits']}å‘½ä¸­")
            
        except Exception as e:
            print(f"âŒ Agentå†³ç­–å¤±è´¥: {e}")
            action = available_actions[0] if available_actions else "look"
            api_time = time.time() - start_time
        
        actions_taken.append(action)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        observation, reward, done, info = env.step(action)
        total_reward += reward
        step_count += 1
        
        print(f"æ‰§è¡Œç»“æœ: å¥–åŠ±={reward}, å®Œæˆ={done}")
        print(f"æ–°è§‚å¯Ÿ: {observation}")
        
        if done and reward > 0:
            print(f"ğŸ‰ {agent_name} æˆåŠŸå®Œæˆä»»åŠ¡ï¼")
            break
        elif reward < 0:
            print(f"âš ï¸ è´Ÿå¥–åŠ±ï¼Œå¯èƒ½æ˜¯é”™è¯¯åŠ¨ä½œ")
    
    success = done and total_reward > 0
    
    print(f"\nğŸ“Š Episode {episode_num} ç»“æœ:")
    print(f"  æˆåŠŸ: {success}")
    print(f"  æ€»æ­¥æ•°: {step_count}")
    print(f"  æ€»å¥–åŠ±: {total_reward}")
    print(f"  åŠ¨ä½œåºåˆ—: {actions_taken}")
    
    # åˆ†æåŠ¨ä½œæ¨¡å¼
    unique_actions = len(set(actions_taken))
    print(f"  åŠ¨ä½œå¤šæ ·æ€§: {unique_actions}/{len(actions_taken)} ({unique_actions/len(actions_taken):.1%})")
    
    if len(actions_taken) > 5:
        last_5 = actions_taken[-5:]
        most_common = max(set(last_5), key=last_5.count)
        count = last_5.count(most_common)
        if count > 3:
            print(f"  âš ï¸ å¯èƒ½é™·å…¥å¾ªç¯: '{most_common}' åœ¨æœ€å5æ­¥ä¸­å‡ºç°{count}æ¬¡")
    
    return {
        'success': success,
        'total_steps': step_count,
        'total_reward': total_reward,
        'actions_taken': actions_taken,
        'agent_type': agent_name
    }

def test_three_agents():
    """æµ‹è¯•ä¸‰ç§Agentçš„è¡¨ç°"""
    print("ğŸ”¬ ä¸‰ç§Agentå¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    print("å¯¹æ¯”: Baseline vs Fixed RAG vs Original RAG")
    
    # åˆ›å»ºç¯å¢ƒ
    env = TextWorldEnvironment('agent_comparison', {
        'difficulty': 'easy',
        'max_episode_steps': 15  # å‡å°‘æ­¥æ•°ï¼Œä¸“æ³¨äºæ•ˆç‡
    })
    
    # åˆ›å»ºå¢å¼ºçŸ¥è¯†å›¾è°±
    print("\nğŸ“š æ„å»ºå¢å¼ºçŸ¥è¯†å›¾è°±...")
    kg = create_enhanced_knowledge_graph()
    retriever = KnowledgeGraphRetriever(kg, 'enhanced_retriever')
    print(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ: {len(kg.facts)} ä¸ªäº‹å®")
    
    # æ˜¾ç¤ºå…³é”®çŸ¥è¯†
    print("å…³é”®çŸ¥è¯†:")
    key_facts = [
        'key location kitchen',
        'go north leads to living room', 
        'go east leads to bedroom',
        'path to bedroom is north then east'
    ]
    for fact in key_facts:
        print(f"  - {fact}")
    
    # åˆ›å»ºä¸‰ç§Agent
    print("\nğŸ¤– åˆ›å»ºä¸‰ç§Agent...")
    
    # 1. Baseline Agent
    baseline_agent = BaselineAgent('baseline_test', {
        'model_name': 'gpt-4o',
        'use_local_model': False,
        'temperature': 0.7,
        'max_tokens': 50
    })
    
    # 2. Fixed RAG Agent
    fixed_rag_agent = FixedRAGAgent('fixed_rag_test', {
        'model_name': 'gpt-4o',
        'use_local_model': False,
        'use_knowledge_graph': True,
        'temperature': 0.7,
        'max_tokens': 80,
        'max_kg_facts': 2
    })
    fixed_rag_agent.set_knowledge_retriever(retriever)
    
    # 3. Original RAG Agent (for comparison)
    original_rag_agent = RAGAgent('original_rag_test', {
        'model_name': 'gpt-4o',
        'use_local_model': False,
        'use_knowledge_graph': True,
        'use_react_reasoning': True,
        'temperature': 0.7,
        'max_tokens': 150,
        'max_kg_facts': 3
    })
    original_rag_agent.set_knowledge_retriever(retriever)
    
    print("âœ… ä¸‰ç§Agentåˆ›å»ºå®Œæˆ")
    
    # è¿è¡Œæµ‹è¯•
    agents = [
        (baseline_agent, "Baseline"),
        (fixed_rag_agent, "Fixed_RAG"),
        (original_rag_agent, "Original_RAG")
    ]
    
    all_results = []
    
    for agent, agent_name in agents:
        print(f"\n{'='*60}")
        print(f"ğŸ”¬ æµ‹è¯• {agent_name}")
        print(f"{'='*60}")
        
        # è¿è¡Œ2ä¸ªepisodes
        for episode in range(2):
            result = run_single_episode_debug(agent, env, agent_name, episode + 1)
            all_results.append(result)
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print(f"\n{'='*60}")
    print("ğŸ“Š æœ€ç»ˆå¯¹æ¯”ç»“æœ")
    print(f"{'='*60}")
    
    for agent, agent_name in agents:
        agent_results = [r for r in all_results if r['agent_type'] == agent_name]
        
        if agent_results:
            success_rate = sum(1 for r in agent_results if r['success']) / len(agent_results)
            avg_steps = sum(r['total_steps'] for r in agent_results) / len(agent_results)
            avg_reward = sum(r['total_reward'] for r in agent_results) / len(agent_results)
            
            print(f"\n{agent_name}:")
            print(f"  Episodes: {len(agent_results)}")
            print(f"  æˆåŠŸç‡: {success_rate:.1%}")
            print(f"  å¹³å‡æ­¥æ•°: {avg_steps:.1f}")
            print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.3f}")
            
            # æ˜¾ç¤ºæˆåŠŸçš„episodes
            successful = [r for r in agent_results if r['success']]
            if successful:
                print(f"  æˆåŠŸæ¡ˆä¾‹åŠ¨ä½œåºåˆ—:")
                for i, r in enumerate(successful):
                    print(f"    Episode {i+1}: {r['actions_taken']}")
    
    return all_results

if __name__ == "__main__":
    print("ğŸš€ ä¿®å¤RAG Agentæµ‹è¯•")
    print("è¿™å°†æµ‹è¯•ä¿®å¤åçš„RAG Agentæ˜¯å¦èƒ½è¶…è¶ŠBaseline")
    
    confirm = input("\nç¡®è®¤å¼€å§‹æµ‹è¯•? (y/N): ").strip().lower()
    if confirm == 'y':
        results = test_three_agents()
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
    else:
        print("æµ‹è¯•å–æ¶ˆ")
