"""
çœŸå®RAG vs Baselineå¯¹æ¯”å®éªŒ
ä½¿ç”¨çœŸå®çš„RAG Agentå’ŒçŸ¥è¯†å›¾è°±è¿›è¡Œå¯¹æ¯”å®éªŒ
"""

import sys
import time
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from src.agents.baseline_agent import BaselineAgent
from src.agents.rag_agent import RAGAgent
from src.environments.textworld_env import TextWorldEnvironment
from src.knowledge.kg_builder import KnowledgeGraphBuilder
from src.knowledge.kg_retriever import KnowledgeGraphRetriever
from src.utils.experiment_logger import ExperimentLogger
from src.utils.logger import get_logger

def create_game_knowledge_graph():
    """åˆ›å»ºæ¸¸æˆç›¸å…³çš„çŸ¥è¯†å›¾è°±"""
    kg = KnowledgeGraphBuilder('game_knowledge')
    
    # æˆ¿é—´å’Œç‰©å“å…³ç³»
    kg.add_fact('kitchen', 'contains', 'fridge')
    kg.add_fact('kitchen', 'contains', 'table')
    kg.add_fact('kitchen', 'has_exit', 'north')
    
    kg.add_fact('fridge', 'contains', 'apple')
    kg.add_fact('fridge', 'contains', 'milk')
    kg.add_fact('fridge', 'can_be', 'opened')
    
    kg.add_fact('bedroom', 'contains', 'chest')
    kg.add_fact('bedroom', 'contains', 'bed')
    kg.add_fact('bedroom', 'has_exit', 'south')
    
    kg.add_fact('chest', 'requires', 'key')
    kg.add_fact('chest', 'contains', 'treasure')
    kg.add_fact('chest', 'can_be', 'opened')
    
    kg.add_fact('key', 'opens', 'chest')
    kg.add_fact('key', 'location', 'kitchen')
    kg.add_fact('key', 'can_be', 'taken')
    
    # åŠ¨ä½œç›¸å…³çŸ¥è¯†
    kg.add_fact('take', 'works_with', 'key')
    kg.add_fact('take', 'works_with', 'apple')
    kg.add_fact('open', 'works_with', 'chest')
    kg.add_fact('open', 'works_with', 'fridge')
    kg.add_fact('go', 'works_with', 'north')
    kg.add_fact('go', 'works_with', 'south')
    
    # ç›®æ ‡ç›¸å…³çŸ¥è¯†
    kg.add_fact('goal', 'requires', 'key')
    kg.add_fact('goal', 'requires', 'chest')
    kg.add_fact('goal', 'action', 'open_chest')
    
    return kg

def run_agent_experiment(agent, env, exp_logger, agent_type, num_episodes=10):
    """è¿è¡Œå•ä¸ªAgentçš„å®éªŒ"""
    logger = get_logger(f"{agent_type}Experiment")
    
    for episode in range(num_episodes):
        logger.info(f"å¼€å§‹ {agent_type} Episode {episode + 1}/{num_episodes}")
        
        # å¼€å§‹è®°å½•episode
        episode_id = exp_logger.start_episode(
            agent_type=agent_type,
            model_name=getattr(agent, 'model_name', 'gpt-4o'),
            environment="TextWorld_Simulated",
            difficulty="easy",
            config={
                'use_knowledge_graph': getattr(agent, 'use_knowledge_graph', False),
                'use_react_reasoning': getattr(agent, 'use_react_reasoning', False),
                'temperature': getattr(agent, 'temperature', 0.7),
                'max_tokens': getattr(agent, 'max_tokens', 150)
            }
        )
        
        # é‡ç½®ç¯å¢ƒå’ŒAgent
        observation = env.reset()
        agent.reset()
        
        total_reward = 0
        done = False
        step_count = 0
        max_steps = 30
        
        logger.debug(f"åˆå§‹è§‚å¯Ÿ: {observation}")
        
        while not done and step_count < max_steps:
            # è·å–å¯ç”¨åŠ¨ä½œ
            available_actions = env.get_available_actions()
            logger.debug(f"å¯ç”¨åŠ¨ä½œ: {available_actions}")
            
            # Agentå†³ç­–
            start_time = time.time()
            try:
                action = agent.act(observation, available_actions)
                api_time = time.time() - start_time
                is_valid = action in available_actions
                
                logger.debug(f"Agenté€‰æ‹©: {action} (æœ‰æ•ˆ: {is_valid})")
                
            except Exception as e:
                logger.error(f"Agentå†³ç­–å¤±è´¥: {e}")
                action = available_actions[0] if available_actions else "look"
                api_time = time.time() - start_time
                is_valid = False
            
            # è®°å½•åŠ¨ä½œ
            exp_logger.log_action(action, is_valid, api_time)
            
            # å¦‚æœæ˜¯RAG Agentï¼Œè®°å½•KGæŸ¥è¯¢
            if hasattr(agent, 'kg_queries') and hasattr(agent, 'kg_hits'):
                if step_count == 0:  # è®°å½•åˆå§‹çŠ¶æ€
                    initial_kg_queries = agent.kg_queries
                    initial_kg_hits = agent.kg_hits
                else:
                    # è®°å½•æ–°çš„KGæŸ¥è¯¢
                    new_queries = agent.kg_queries - getattr(agent, '_last_kg_queries', 0)
                    new_hits = agent.kg_hits - getattr(agent, '_last_kg_hits', 0)
                    if new_queries > 0:
                        exp_logger.log_kg_query(f"step_{step_count}", new_hits)
                
                agent._last_kg_queries = agent.kg_queries
                agent._last_kg_hits = agent.kg_hits
            
            # æ‰§è¡ŒåŠ¨ä½œ
            observation, reward, done, info = env.step(action)
            total_reward += reward
            step_count += 1
            
            logger.debug(f"Step {step_count}: å¥–åŠ±={reward}, å®Œæˆ={done}, æ–°è§‚å¯Ÿ={observation[:100]}...")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if done and reward > 0:
                logger.info(f"ğŸ‰ {agent_type} æˆåŠŸå®Œæˆä»»åŠ¡ï¼")
                break
        
        # åˆ¤æ–­æˆåŠŸ
        success = done and total_reward > 0
        
        # ç»“æŸepisodeè®°å½•
        result = exp_logger.end_episode(success, total_reward)
        
        logger.info(f"{agent_type} Episode {episode + 1} å®Œæˆ: "
                   f"æˆåŠŸ={success}, æ­¥æ•°={result.total_steps}, å¥–åŠ±={total_reward:.3f}")
        
        # å¦‚æœæ˜¯RAG Agentï¼Œæ˜¾ç¤ºKGä½¿ç”¨ç»Ÿè®¡
        if hasattr(agent, 'get_stats'):
            stats = agent.get_stats()
            if stats.get('kg_queries', 0) > 0:
                logger.info(f"  KGæŸ¥è¯¢: {stats['kg_queries']}, å‘½ä¸­: {stats['kg_hits']}, "
                           f"å‘½ä¸­ç‡: {stats['kg_hit_rate']:.2%}")

def run_real_rag_vs_baseline_experiment(baseline_episodes=10, rag_episodes=10):
    """è¿è¡ŒçœŸå®çš„RAG vs Baselineå¯¹æ¯”å®éªŒ"""
    logger = get_logger("RealRAGExperiment")
    
    print("ğŸ§ª å¼€å§‹çœŸå®RAG vs Baselineå¯¹æ¯”å®éªŒ")
    print("=" * 60)
    
    # åˆ›å»ºå®éªŒè®°å½•å™¨
    exp_logger = ExperimentLogger("real_rag_vs_baseline")
    
    # åˆ›å»ºç¯å¢ƒ
    env = TextWorldEnvironment('real_rag_exp', {
        'difficulty': 'easy',
        'max_episode_steps': 30
    })
    
    # åˆ›å»ºçŸ¥è¯†å›¾è°±
    print("ğŸ“š æ„å»ºæ¸¸æˆçŸ¥è¯†å›¾è°±...")
    kg = create_game_knowledge_graph()
    retriever = KnowledgeGraphRetriever(kg, 'game_retriever')
    print(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ: {len(kg.facts)} ä¸ªäº‹å®")
    
    # åˆ›å»ºBaseline Agent
    print("ğŸ¤– åˆ›å»ºBaseline Agent...")
    baseline_agent = BaselineAgent('baseline_real', {
        'model_name': 'gpt-4o',
        'use_local_model': False,
        'temperature': 0.7,
        'max_tokens': 150
    })
    
    # åˆ›å»ºRAG Agent
    print("ğŸ§  åˆ›å»ºRAG Agent...")
    rag_agent = RAGAgent('rag_real', {
        'model_name': 'gpt-4o',
        'use_local_model': False,
        'use_knowledge_graph': True,
        'use_react_reasoning': True,
        'temperature': 0.7,
        'max_tokens': 200,
        'max_kg_facts': 5,
        'kg_retrieval_method': 'semantic'
    })
    
    # è®¾ç½®RAG Agentçš„çŸ¥è¯†æ£€ç´¢å™¨
    rag_agent.set_knowledge_retriever(retriever)
    print("âœ… RAG AgentçŸ¥è¯†æ£€ç´¢å™¨è®¾ç½®å®Œæˆ")
    
    # è¿è¡ŒBaselineå®éªŒ
    print(f"\nğŸ”¬ è¿è¡ŒBaseline Agentå®éªŒ ({baseline_episodes} episodes)...")
    run_agent_experiment(baseline_agent, env, exp_logger, "Baseline_LLM_Real", baseline_episodes)
    
    # è¿è¡ŒRAGå®éªŒ
    print(f"\nğŸ”¬ è¿è¡ŒRAG Agentå®éªŒ ({rag_episodes} episodes)...")
    run_agent_experiment(rag_agent, env, exp_logger, "RAG_LLM_Real", rag_episodes)
    
    # ä¿å­˜ç»“æœ
    print("\nğŸ“Š ä¿å­˜å®éªŒç»“æœ...")
    json_file, csv_file = exp_logger.save_results()
    
    # ç”ŸæˆæŠ¥å‘Šå’Œå›¾è¡¨
    print("ğŸ“ˆ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    summary_report = exp_logger.generate_summary_report()
    comparison_plot = exp_logger.generate_comparison_plots()
    detailed_plot = exp_logger.generate_detailed_analysis()
    
    print("\n" + "="*60)
    print("ğŸ‰ çœŸå®RAG vs Baselineå®éªŒå®Œæˆ!")
    print("="*60)
    print(summary_report)
    
    print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
    print(f"  æ•°æ®: {json_file}")
    print(f"  CSV: {csv_file}")
    print(f"  å¯¹æ¯”å›¾: {comparison_plot}")
    print(f"  è¯¦ç»†åˆ†æ: {detailed_plot}")
    
    return exp_logger

if __name__ == "__main__":
    print("ğŸš€ çœŸå®RAG Agentå®éªŒ")
    print("è¿™å°†ä½¿ç”¨çœŸå®çš„:")
    print("  âœ… GPT-4o APIè°ƒç”¨")
    print("  âœ… çŸ¥è¯†å›¾è°±æ£€ç´¢")
    print("  âœ… ReActæ¨ç†æ¡†æ¶")
    print("  âœ… å®Œæ•´çš„RAGç®¡é“")
    
    confirm = input("\nç¡®è®¤å¼€å§‹å®éªŒ? (y/N): ").strip().lower()
    if confirm == 'y':
        run_real_rag_vs_baseline_experiment(baseline_episodes=8, rag_episodes=8)
    else:
        print("å®éªŒå–æ¶ˆ")
