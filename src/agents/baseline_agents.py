#!/usr/bin/env python3
"""
åŸºçº¿æ™ºèƒ½ä½“å®ç°
åŒ…å«LLMåŸºçº¿ã€ReAct Agentã€RAG Agentä¸‰æ¡å¯¹æ¯”çº¿
"""

import json
import random
import re
import os
from typing import Dict, Any, List, Tuple, Optional
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥ç®€åŒ–é…ç½®ç®¡ç†å™¨
from src.utils.simple_config import get_config

# å°è¯•å¯¼å…¥OpenAI
try:
    import openai
    OPENAI_AVAILABLE = True

    # ä»ç®€åŒ–é…ç½®è·å–APIå¯†é’¥å’Œbase_url
    try:
        config = get_config()
        api_key = config.get_api_key()
        base_url = config.get_base_url()

        if api_key and api_key.startswith('sk-'):
            openai.api_key = api_key
            print(f"âœ… OpenAI API key loaded: {api_key[:10]}...")
            print(f"âœ… API base URL: {base_url}")
        else:
            print("âš ï¸ API key not found or invalid, using simulated responses")
            OPENAI_AVAILABLE = False

    except Exception as e:
        print(f"âš ï¸ Failed to load config: {e}, using simulated responses")
        OPENAI_AVAILABLE = False

except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸ OpenAI not available, using simulated responses")


class LLMBaselineAgent:
    """
    LLMåŸºçº¿æ™ºèƒ½ä½“
    å‚è€ƒ: TextWorld (CÃ´tÃ© et al., 2019), ALFWorld (Shridhar et al., 2020)
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        if config is None:
            config = {}
        self.model_name = config.get('model', 'claude-3-5-sonnet-20241022')
        self.action_history = []
        self.observation_history = []
        self.max_history = 10

        print(f"ğŸ¤– åˆå§‹åŒ–LLMåŸºçº¿æ™ºèƒ½ä½“ (æ¨¡å‹: {self.model_name})")
    
    def reset(self, scene_info: Dict[str, Any] = None):
        """é‡ç½®æ™ºèƒ½ä½“"""
        self.action_history = []
        self.observation_history = []
    
    def select_action(self, observation: Dict[str, Any]) -> Tuple[str, str]:
        """é€‰æ‹©åŠ¨ä½œ - ä½¿ç”¨çœŸå®LLMæˆ–æ¨¡æ‹Ÿå®ç°"""
        print(f"\nğŸ¤– LLM Baseline Agent - Processing observation...")
        print(f"ğŸ“¥ Raw observation: {observation}")

        # è®°å½•è§‚å¯Ÿ
        self.observation_history.append(observation)
        if len(self.observation_history) > self.max_history:
            self.observation_history.pop(0)

        # ç®€å•çš„LLMåŸºçº¿ç­–ç•¥
        available_actions = observation.get('available_actions', [])
        visible_entities = observation.get('visible_entities', [])
        inventory = observation.get('agent_inventory', [])

        print(f"ğŸ” Extracted info:")
        print(f"  - Available actions: {available_actions}")
        print(f"  - Visible entities: {visible_entities}")
        print(f"  - Inventory: {inventory}")

        # ä½¿ç”¨çœŸå®LLMæˆ–æ¨¡æ‹Ÿæ¨ç†
        if OPENAI_AVAILABLE and openai.api_key:
            action, target = self._call_real_llm(observation, available_actions, visible_entities, inventory)
        else:
            action, target = self._simulate_llm_reasoning(observation, available_actions, visible_entities, inventory)

        # è®°å½•åŠ¨ä½œ
        self.action_history.append((action, target))
        if len(self.action_history) > self.max_history:
            self.action_history.pop(0)

        print(f"ğŸ¯ LLM Decision: {action} -> {target}")
        return action, target

    def _call_real_llm(self, observation: Dict[str, Any], available_actions: List[str],
                      visible_entities: List[str], inventory: List[str]) -> Tuple[str, str]:
        """è°ƒç”¨çœŸå®çš„LLMè¿›è¡Œæ¨ç†"""
        print(f"ğŸ”¥ Calling real LLM ({self.model_name})...")

        # æ„å»ºæç¤º
        prompt = self._build_llm_prompt(observation, available_actions, visible_entities, inventory)
        print(f"ğŸ“ LLM Prompt:")
        print(f"{'='*60}")
        print(prompt)
        print(f"{'='*60}")

        try:
            # è°ƒç”¨OpenAI API (æ–°ç‰ˆæœ¬)
            from openai import OpenAI

            # è·å–é…ç½®
            config = get_config()
            base_url = config.get_base_url()

            # åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œæ”¯æŒè‡ªå®šä¹‰base_url
            client = OpenAI(
                api_key=openai.api_key,
                base_url=base_url
            )

            response = client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are an AI agent in a household environment. You need to select actions to complete tasks."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=150,
                temperature=0.3  # é™ä½éšæœºæ€§
            )

            raw_response = response.choices[0].message.content.strip()
            print(f"ğŸ¤– Raw LLM Response:")
            print(f"{'='*60}")
            print(raw_response)
            print(f"{'='*60}")

            # è§£æå“åº”
            action, target = self._parse_llm_response(raw_response, available_actions, visible_entities)
            print(f"âœ… Parsed action: {action} -> {target}")

            return action, target

        except Exception as e:
            print(f"âŒ LLM call failed: {e}")
            print("ğŸš« No fallback - experiment requires real LLM responses")
            raise e  # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸ä½¿ç”¨æ¨¡æ‹Ÿ

    def _build_llm_prompt(self, observation: Dict[str, Any], available_actions: List[str],
                         visible_entities: List[str], inventory: List[str]) -> str:
        """æ„å»ºLLMæç¤º"""
        prompt = f"""You are an AI agent in a household environment.

Current situation:
- Available actions: {available_actions}
- Visible entities: {visible_entities}
- Your inventory: {inventory}
- Previous actions: {self.action_history[-3:] if self.action_history else 'None'}

TASK OBJECTIVES:
1. PRIMARY: Find and collect useful items (keys, tools, food items)
2. SECONDARY: Explore systematically to discover new areas and objects
3. TERTIARY: Interact with containers (cabinets, drawers) to find hidden items

STRATEGY:
- Examine objects before picking them up to understand their properties
- Prioritize keys and tools as they enable access to more areas
- Use go_to to move between different areas for exploration
- Open containers when you have appropriate keys

Please select ONE action and ONE target from the available options.
Respond in the format: "ACTION: <action_name> TARGET: <target_name>"

For example:
- ACTION: go_to TARGET: Cabinet_123
- ACTION: examine TARGET: Bed_456
- ACTION: pick_up TARGET: Apple_789

Choose wisely based on the current situation and task objectives."""

        return prompt

    def _parse_llm_response(self, response: str, available_actions: List[str],
                           visible_entities: List[str]) -> Tuple[str, str]:
        """è§£æLLMå“åº”"""
        # å°è¯•æå–ACTIONå’ŒTARGET
        action_match = re.search(r'ACTION:\s*(\w+)', response, re.IGNORECASE)
        target_match = re.search(r'TARGET:\s*([^\s\n]+)', response, re.IGNORECASE)

        if action_match and target_match:
            action = action_match.group(1).lower()
            target = target_match.group(1)

            # éªŒè¯åŠ¨ä½œæ˜¯å¦å¯ç”¨
            if action in available_actions:
                # éªŒè¯ç›®æ ‡æ˜¯å¦å¯è§ï¼ˆå¯¹äºéœ€è¦ç›®æ ‡çš„åŠ¨ä½œï¼‰
                if action in ['go_to', 'examine', 'pick_up', 'open', 'close'] and target in visible_entities:
                    return action, target
                elif action in ['wait']:
                    return action, None

        # å¦‚æœè§£æå¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
        print(f"âš ï¸ Failed to parse LLM response")
        raise ValueError(f"Cannot parse LLM response: {response}")


    
    def update(self, observation: Dict[str, Any], action: str, target: str, 
               reward: float, next_observation: Dict[str, Any], done: bool):
        """æ›´æ–°æ™ºèƒ½ä½“ï¼ˆåŸºçº¿ä¸å­¦ä¹ ï¼‰"""
        pass
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'agent_type': 'llm_baseline',
            'model_name': self.model_name,
            'total_actions': len(self.action_history),
            'history_length': len(self.observation_history)
        }


class ReActAgent:
    """
    ReActæ™ºèƒ½ä½“ - ä½¿ç”¨çœŸå®LLMè¿›è¡ŒThink-Act-Observeæ¨ç†
    å‚è€ƒ: ReAct (Yao et al., 2022), Reflexion (Shinn et al., 2023)
    """

    def __init__(self, config: Dict[str, Any] = None):
        if config is None:
            config = {}
        self.thought_history = []
        self.action_history = []
        self.observation_history = []
        self.reasoning_steps = []
        self.max_reasoning_steps = config.get('max_reasoning_steps', 5)
        self.model_name = config.get('model', 'claude-3-5-sonnet-20241022')

        print(f"ğŸ§  åˆå§‹åŒ–ReActæ™ºèƒ½ä½“ (æ¨¡å‹: {self.model_name})")

    def reset(self, scene_info: Dict[str, Any] = None):
        """é‡ç½®æ™ºèƒ½ä½“"""
        self.thought_history = []
        self.action_history = []
        self.observation_history = []
        self.reasoning_steps = []
    
    def select_action(self, observation: Dict[str, Any]) -> Tuple[str, str]:
        """ReAct: Reasoning + Acting - ä½¿ç”¨çœŸå®LLM"""
        print(f"\nğŸ§  ReAct Agent - Starting Think-Act-Observe cycle...")
        print(f"ğŸ“¥ Raw observation: {observation}")

        # è®°å½•è§‚å¯Ÿ
        self.observation_history.append(observation)

        # æå–è§‚å¯Ÿä¿¡æ¯
        available_actions = observation.get('available_actions', [])
        visible_entities = observation.get('visible_entities', [])
        inventory = observation.get('agent_inventory', [])

        # ä½¿ç”¨LLMè¿›è¡ŒReActæ¨ç†
        action, target = self._llm_react_reasoning(observation, available_actions, visible_entities, inventory)

        # è®°å½•åŠ¨ä½œå†å²
        self.action_history.append((action, target))

        print(f"ğŸ“‹ ReAct reasoning complete. Next: OBSERVE phase (after action execution)")
        return action, target

    def _llm_react_reasoning(self, observation: Dict[str, Any], available_actions: List[str],
                           visible_entities: List[str], inventory: List[str]) -> Tuple[str, str]:
        """ä½¿ç”¨çœŸå®LLMè¿›è¡ŒReActæ¨ç†"""
        print(f"ğŸ”¥ Calling real LLM for ReAct reasoning ({self.model_name})...")

        # æ„å»ºReActæç¤º
        prompt = self._build_react_prompt(observation, available_actions, visible_entities, inventory)
        print(f"ğŸ“ ReAct LLM Prompt:")
        print(f"{'='*60}")
        print(prompt)
        print(f"{'='*60}")

        try:
            # è°ƒç”¨OpenAI API
            from openai import OpenAI
            from src.utils.simple_config import SimpleConfig

            config = SimpleConfig()
            api_key = config.get_api_key()
            base_url = config.get_base_url()

            client = OpenAI(
                api_key=api_key,
                base_url=base_url
            )

            response = client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are an AI agent using ReAct (Reasoning + Acting) methodology. Think step by step, then act."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=200,
                temperature=0.3  # é™ä½éšæœºæ€§
            )

            raw_response = response.choices[0].message.content.strip()
            print(f"ğŸ¤– Raw ReAct LLM Response:")
            print(f"{'='*60}")
            print(raw_response)
            print(f"{'='*60}")

            # è§£æReActå“åº”
            action, target = self._parse_react_response(raw_response, available_actions, visible_entities)
            print(f"âœ… Parsed ReAct action: {action} -> {target}")

            return action, target

        except Exception as e:
            print(f"âŒ ReAct LLM call failed: {e}")
            print("ğŸš« No fallback - experiment requires real LLM responses")
            raise e

    def _build_react_prompt(self, observation: Dict[str, Any], available_actions: List[str],
                          visible_entities: List[str], inventory: List[str]) -> str:
        """æ„å»ºReActæç¤º"""
        # è·å–å†å²æ¨ç†æ­¥éª¤
        recent_history = ""
        if self.reasoning_steps:
            recent_history = "\nRecent reasoning history:\n"
            for i, step in enumerate(self.reasoning_steps[-3:]):
                recent_history += f"Step {i+1}: Thought: {step.get('thought', 'N/A')} -> Action: {step.get('action', 'N/A')} {step.get('target', 'N/A')}\n"

        prompt = f"""You are using ReAct (Reasoning + Acting) methodology in a household environment.

Current situation:
- Available actions: {available_actions}
- Visible entities: {visible_entities}
- Your inventory: {inventory}
- Previous actions: {self.action_history[-3:] if self.action_history else 'None'}{recent_history}

TASK OBJECTIVES:
1. PRIMARY: Find and collect useful items (keys, tools, food items)
2. SECONDARY: Explore systematically to discover new areas and objects
3. TERTIARY: Interact with containers (cabinets, drawers) to find hidden items

ReAct Process:
1. THINK: Analyze the current situation, consider task objectives, and reason about what to do next
2. ACT: Choose one specific action based on your reasoning that advances the task objectives

Please follow this format:
THINK: [Your reasoning about the current situation, task progress, and what you should do next]
ACT: ACTION: <action_name> TARGET: <target_name>

For example:
THINK: I can see a coffee table and an armchair. Since my primary objective is to find useful items, I should examine the coffee table first to see if there are any keys or tools on it before exploring other areas.
ACT: ACTION: examine TARGET: CoffeeTable_992

Now, think and act based on the current situation and task objectives:"""

        return prompt

    def _parse_react_response(self, response: str, available_actions: List[str],
                            visible_entities: List[str]) -> Tuple[str, str]:
        """è§£æReActå“åº”"""
        # æå–THINKéƒ¨åˆ†
        think_match = re.search(r'THINK:\s*(.+?)(?=ACT:|$)', response, re.DOTALL | re.IGNORECASE)
        if think_match:
            thought = think_match.group(1).strip()
            print(f"ğŸ’­ LLM Thought: {thought}")
            self.thought_history.append(thought)

        # æå–ACTIONå’ŒTARGET
        action_match = re.search(r'ACTION:\s*(\w+)', response, re.IGNORECASE)
        target_match = re.search(r'TARGET:\s*([^\s\n]+)', response, re.IGNORECASE)

        if action_match and target_match:
            action = action_match.group(1).lower()
            target = target_match.group(1)

            # éªŒè¯åŠ¨ä½œå’Œç›®æ ‡
            if action in available_actions and target in visible_entities:
                return action, target
            else:
                print(f"âš ï¸ Invalid action/target, using fallback")

        # å›é€€ç­–ç•¥
        if available_actions and visible_entities:
            return available_actions[0], visible_entities[0]

        raise ValueError(f"Cannot parse ReAct response: {response}")

    def _generate_thought(self, observation: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ€è€ƒ"""
        current_location = observation.get('agent_location', 'unknown')
        inventory = observation.get('agent_inventory', [])
        visible_entities = observation.get('visible_entities', [])
        
        # æ¨¡æ‹ŸReActçš„æ€è€ƒè¿‡ç¨‹
        if not inventory:
            thought = f"I'm at {current_location} with no items. I should look for useful objects to pick up."
        elif inventory and any('key' in item.lower() for item in inventory):
            thought = f"I have a key: {inventory}. I should look for something to unlock."
        elif visible_entities:
            thought = f"I can see: {visible_entities}. I should examine or interact with them."
        else:
            thought = "I need to explore more to find useful items or locations."
        
        return thought
    
    def _reason_and_act(self, observation: Dict[str, Any], thought: str) -> Tuple[str, str]:
        """åŸºäºæ€è€ƒè¿›è¡Œæ¨ç†å’Œè¡ŒåŠ¨"""
        available_actions = observation.get('available_actions', [])
        visible_entities = observation.get('visible_entities', [])
        inventory = observation.get('agent_inventory', [])
        
        # ReActæ¨ç†é“¾
        if "look for useful objects" in thought and 'pick_up' in available_actions:
            useful_items = [e for e in visible_entities if any(keyword in e.lower() 
                           for keyword in ['key', 'treasure', 'egg', 'apple'])]
            if useful_items:
                return "pick_up", useful_items[0]
        
        elif "look for something to unlock" in thought and 'open' in available_actions:
            lockable_items = [e for e in visible_entities if any(keyword in e.lower() 
                             for keyword in ['cabinet', 'drawer', 'chest', 'safe'])]
            if lockable_items:
                return "open", lockable_items[0]
        
        elif "examine or interact" in thought and 'examine' in available_actions:
            return "examine", visible_entities[0] if visible_entities else None
        
        elif "explore more" in thought and 'go_to' in available_actions:
            current_location = observation.get('agent_location', '')
            new_locations = [e for e in visible_entities if e != current_location]
            if new_locations:
                return "go_to", new_locations[0]
        
        # é»˜è®¤è¡Œä¸º
        if available_actions:
            return available_actions[0], visible_entities[0] if visible_entities else None
        
        return "wait", None
    
    def update(self, observation: Dict[str, Any], action: str, target: str, 
               reward: float, next_observation: Dict[str, Any], done: bool):
        """æ›´æ–°æ™ºèƒ½ä½“ - ReActå¯ä»¥è¿›è¡Œåæ€"""
        # ç®€å•çš„åæ€æœºåˆ¶
        if reward < -0.1:  # è´Ÿå¥–åŠ±
            reflection = f"Action {action} on {target} resulted in negative reward {reward}. Should avoid similar actions."
            self.thought_history.append(f"Reflection: {reflection}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'agent_type': 'react',
            'total_thoughts': len(self.thought_history),
            'total_actions': len(self.action_history),
            'reasoning_steps': len(self.reasoning_steps),
            'recent_reasoning': self.reasoning_steps[-3:] if self.reasoning_steps else []
        }


class RAGAgent:
    """
    RAG (Retrieval-Augmented Generation) æ™ºèƒ½ä½“ - ä½¿ç”¨çœŸå®LLMè¿›è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆ
    å‚è€ƒ: RAG (Lewis et al., 2020), WebGPT (Nakano et al., 2021)
    """

    def __init__(self, config: Dict[str, Any] = None):
        if config is None:
            config = {}
        self.knowledge_base = {}
        self.scene_kg = {}  # å­˜å‚¨åœºæ™¯ç‰¹å®šçš„KG
        self.retrieval_history = []
        self.action_history = []
        self.max_retrievals = config.get('max_retrievals', 5)
        self.model_name = config.get('model', 'gpt-3.5-turbo')
        self.knowledge_base_path = config.get('knowledge_base_path')
        self.current_scene = None

        # åŠ è½½çŸ¥è¯†åº“
        if self.knowledge_base_path:
            self._load_knowledge_base(self.knowledge_base_path)
        else:
            self._create_default_knowledge_base()

        print(f"ğŸ” åˆå§‹åŒ–RAGæ™ºèƒ½ä½“ (æ¨¡å‹: {self.model_name}, çŸ¥è¯†åº“è·¯å¾„: {self.knowledge_base_path})")
        print(f"ğŸ“š é»˜è®¤çŸ¥è¯†åº“æ¡ç›®: {len(self.knowledge_base)}")
    
    def _load_knowledge_base(self, kb_path: str):
        """åŠ è½½å¤–éƒ¨çŸ¥è¯†åº“"""
        try:
            # å¦‚æœæ˜¯ç›®å½•ï¼Œå…ˆåˆ›å»ºé»˜è®¤çŸ¥è¯†åº“
            if os.path.isdir(kb_path):
                print(f"ğŸ“ çŸ¥è¯†åº“è·¯å¾„æ˜¯ç›®å½•: {kb_path}")
                self._create_default_knowledge_base()
                return

            # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½
            with open(kb_path, 'r', encoding='utf-8') as f:
                self.knowledge_base = json.load(f)
                print(f"âœ… æˆåŠŸåŠ è½½çŸ¥è¯†åº“æ–‡ä»¶: {kb_path}")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
            self._create_default_knowledge_base()
    
    def _create_default_knowledge_base(self):
        """åˆ›å»ºé»˜è®¤çŸ¥è¯†åº“"""
        self.knowledge_base = {
            'object_interactions': {
                'key': ['Can unlock doors, cabinets, drawers', 'Usually found on tables or in containers'],
                'cabinet': ['Can be opened with keys', 'May contain useful items', 'Check if locked first'],
                'drawer': ['Can be opened', 'Often contains small items', 'May require key'],
                'apple': ['Food item', 'Can be eaten', 'Often found in kitchen areas'],
                'egg': ['Fragile food item', 'Can be cooked', 'Handle carefully'],
                'microwave': ['Cooking appliance', 'Can heat food', 'Need to open first']
            },
            'action_strategies': {
                'pick_up': ['Pick up useful items first', 'Check inventory space', 'Prioritize keys and tools'],
                'open': ['Try keys on locked containers', 'Check if already open', 'Look inside after opening'],
                'go_to': ['Explore systematically', 'Visit kitchen for food items', 'Check all rooms'],
                'examine': ['Get detailed information', 'Understand object properties', 'Plan next actions']
            },
            'task_patterns': {
                'cooking_task': ['Find ingredients', 'Use cooking appliances', 'Follow recipe steps'],
                'cleaning_task': ['Find cleaning supplies', 'Clean systematically', 'Put items away'],
                'retrieval_task': ['Locate target item', 'Navigate efficiently', 'Handle carefully']
            }
        }
    
    def reset(self, scene_info: Dict[str, Any] = None):
        """é‡ç½®æ™ºèƒ½ä½“"""
        self.retrieval_history = []
        self.action_history = []

        # åŠ è½½åœºæ™¯ç‰¹å®šçš„KG
        if scene_info and 'scene_name' in scene_info:
            self.current_scene = scene_info['scene_name']
            self._load_scene_kg(self.current_scene)

    def _load_scene_kg(self, scene_name: str):
        """åŠ è½½åœºæ™¯ç‰¹å®šçš„çŸ¥è¯†å›¾è°±"""
        if not self.knowledge_base_path or not os.path.isdir(self.knowledge_base_path):
            print(f"âš ï¸  æ— æ³•åŠ è½½åœºæ™¯KG: çŸ¥è¯†åº“è·¯å¾„æ— æ•ˆ")
            return

        # å°è¯•åŠ è½½åŠ¨ä½œ-çŠ¶æ€åœºæ™¯KG
        kg_file = os.path.join(self.knowledge_base_path, f"{scene_name}_action_state_kg.json")
        if not os.path.exists(kg_file):
            print(f"âš ï¸  åœºæ™¯KGæ–‡ä»¶ä¸å­˜åœ¨: {kg_file}")
            return

        try:
            with open(kg_file, 'r', encoding='utf-8') as f:
                self.scene_kg = json.load(f)
                print(f"âœ… æˆåŠŸåŠ è½½åœºæ™¯KG: {kg_file}")
                print(f"ğŸ“Š KGèŠ‚ç‚¹æ•°: {len(self.scene_kg.get('nodes', []))}")
                print(f"ğŸ“Š KGè¾¹æ•°: {len(self.scene_kg.get('edges', []))}")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½åœºæ™¯KGå¤±è´¥: {e}")
            self.scene_kg = {}
    
    def select_action(self, observation: Dict[str, Any]) -> Tuple[str, str]:
        """RAG: æ£€ç´¢ç›¸å…³çŸ¥è¯† + ä½¿ç”¨LLMç”ŸæˆåŠ¨ä½œ"""
        # 1. ä»è§‚å¯Ÿä¸­æå–å…³é”®ä¿¡æ¯
        key_entities = self._extract_key_entities(observation)

        # 2. æ£€ç´¢ç›¸å…³çŸ¥è¯†
        retrieved_knowledge = self._retrieve_knowledge(key_entities, observation)

        # 3. ä½¿ç”¨LLMåŸºäºæ£€ç´¢çš„çŸ¥è¯†ç”ŸæˆåŠ¨ä½œ
        available_actions = observation.get('available_actions', [])
        visible_entities = observation.get('visible_entities', [])
        inventory = observation.get('agent_inventory', [])

        action, target = self._llm_rag_generation(observation, retrieved_knowledge,
                                                available_actions, visible_entities, inventory)

        # è®°å½•æ£€ç´¢å†å²
        self.retrieval_history.append({
            'entities': key_entities,
            'knowledge': retrieved_knowledge,
            'action': action,
            'target': target
        })

        if len(self.retrieval_history) > self.max_retrievals:
            self.retrieval_history.pop(0)

        return action, target

    def select_action_with_tracking(self, observation: Dict[str, Any]) -> Tuple[str, str, List[str], str]:
        """RAG: æ£€ç´¢ç›¸å…³çŸ¥è¯† + ç”ŸæˆåŠ¨ä½œï¼Œè¿”å›è¿½è¸ªä¿¡æ¯"""
        print(f"\nğŸ” RAG Agent - Starting Retrieval-Augmented Generation...")
        print(f"ğŸ“¥ Raw observation: {observation}")

        # 1. ä»è§‚å¯Ÿä¸­æå–å…³é”®ä¿¡æ¯
        print(f"ğŸ¯ Step 1: Extracting key entities...")
        key_entities = self._extract_key_entities(observation)
        print(f"ğŸ“‹ Extracted entities: {key_entities}")

        # 2. æ£€ç´¢ç›¸å…³çŸ¥è¯†
        print(f"ğŸ” Step 2: Retrieving knowledge from KG...")
        retrieved_knowledge = self._retrieve_knowledge(key_entities, observation)
        print(f"ğŸ“š Retrieved knowledge for {len(retrieved_knowledge)} entities:")
        for entity, knowledge in retrieved_knowledge.items():
            print(f"  - {entity}: {len(knowledge) if isinstance(knowledge, list) else 1} items")

        # 3. ä½¿ç”¨çœŸå®LLMåŸºäºæ£€ç´¢çš„çŸ¥è¯†ç”ŸæˆåŠ¨ä½œ
        print(f"ğŸ¬ Step 3: Generating action with retrieved knowledge using real LLM...")
        available_actions = observation.get('available_actions', [])
        visible_entities = observation.get('visible_entities', [])
        inventory = observation.get('agent_inventory', [])
        action, target = self._llm_rag_generation(observation, retrieved_knowledge,
                                                available_actions, visible_entities, inventory)
        print(f"ğŸ¯ Generated action: {action} -> {target}")

        # 4. æ”¶é›†è®¿é—®çš„KGèŠ‚ç‚¹
        print(f"ğŸ“Š Step 4: Collecting accessed KG nodes...")
        kg_nodes_accessed = []
        kg_nodes_accessed.extend(key_entities)  # æå–çš„å®ä½“
        for entity, knowledge_list in retrieved_knowledge.items():
            kg_nodes_accessed.append(entity)
            if isinstance(knowledge_list, list):
                kg_nodes_accessed.extend([k for k in knowledge_list if isinstance(k, str)])

        print(f"ğŸ”— Total KG nodes accessed: {len(kg_nodes_accessed)}")
        print(f"ğŸ“ KG nodes: {kg_nodes_accessed}")

        # 5. ç”Ÿæˆæ¨ç†è½¨è¿¹
        reasoning_trace = f"RAG: Extracted {len(key_entities)} entities: {key_entities[:3]}..., "
        reasoning_trace += f"Retrieved knowledge for {len(retrieved_knowledge)} entities, "
        reasoning_trace += f"Selected action: {action} -> {target}"

        # 6. è®°å½•å†å²
        self.retrieval_history.append({
            'entities': key_entities,
            'knowledge': retrieved_knowledge,
            'action': action,
            'target': target,
            'kg_nodes_accessed': kg_nodes_accessed,
            'reasoning_trace': reasoning_trace
        })

        if len(self.retrieval_history) > self.max_retrievals:
            self.retrieval_history.pop(0)

        return action, target, kg_nodes_accessed, reasoning_trace

    def _extract_key_entities(self, observation: Dict[str, Any]) -> List[str]:
        """ä»è§‚å¯Ÿä¸­æå–å…³é”®å®ä½“ - æ”¹è¿›ç‰ˆæœ¬"""
        key_entities = []

        # 1. ç›´æ¥ä½¿ç”¨å¯è§å®ä½“åç§°
        visible_entities = observation.get('visible_entities', [])
        key_entities.extend(visible_entities)

        # 2. ä»åº“å­˜ä¸­æå–
        inventory = observation.get('agent_inventory', [])
        key_entities.extend(inventory)

        # 3. å¦‚æœæœ‰åœºæ™¯KGï¼Œæå–å®ä½“ç±»å‹ä¿¡æ¯
        if self.scene_kg and 'nodes' in self.scene_kg:
            for entity_name in visible_entities + inventory:
                # åœ¨KGä¸­æŸ¥æ‰¾åŒ¹é…çš„èŠ‚ç‚¹
                for node in self.scene_kg['nodes']:
                    if (node.get('name') == entity_name or
                        entity_name in node.get('name', '') or
                        node.get('name', '') in entity_name):

                        # æ·»åŠ å®ä½“ç±»å‹ä¿¡æ¯
                        attrs = node.get('attributes', {})
                        if 'object_type' in attrs:
                            key_entities.append(attrs['object_type'].lower())
                        if 'entity_type' in attrs:
                            key_entities.append(attrs['entity_type'].lower())

        # 4. æ·»åŠ é€šç”¨ç­–ç•¥å®ä½“ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        key_entities.extend(['examine', 'pick_up', 'open', 'go_to'])

        return list(set(key_entities))  # å»é‡
    
    def _retrieve_knowledge(self, key_entities: List[str], observation: Dict[str, Any]) -> Dict[str, List[str]]:
        """æ£€ç´¢ç›¸å…³çŸ¥è¯† - æ”¹è¿›ç‰ˆæœ¬"""
        retrieved = {}

        # 1. ä»åœºæ™¯KGä¸­æ£€ç´¢å®ä½“ç‰¹å®šçŸ¥è¯†
        if self.scene_kg and 'nodes' in self.scene_kg:
            for entity in key_entities:
                entity_info = self._get_entity_info_from_kg(entity)
                if entity_info:
                    retrieved[f"kg_{entity}"] = entity_info

        # 2. æ£€ç´¢å¯¹è±¡äº¤äº’çŸ¥è¯†ï¼ˆé»˜è®¤çŸ¥è¯†åº“ï¼‰
        for entity in key_entities:
            if entity in self.knowledge_base.get('object_interactions', {}):
                retrieved[f"about_{entity}"] = self.knowledge_base['object_interactions'][entity]

        # 3. æ£€ç´¢åŠ¨ä½œç­–ç•¥çŸ¥è¯†
        available_actions = observation.get('available_actions', [])
        for action in available_actions:
            if action in self.knowledge_base.get('action_strategies', {}):
                retrieved[f"strategy_{action}"] = self.knowledge_base['action_strategies'][action]

        # 4. æ£€ç´¢ä»»åŠ¡æ¨¡å¼çŸ¥è¯†ï¼ˆåŸºäºä¸Šä¸‹æ–‡æ¨æ–­ï¼‰
        context = observation.get('description', '').lower()
        for task_type, strategies in self.knowledge_base.get('task_patterns', {}).items():
            if any(keyword in context for keyword in ['cook', 'kitchen', 'food']) and 'cooking' in task_type:
                retrieved[f"pattern_{task_type}"] = strategies
            elif any(keyword in context for keyword in ['clean', 'dirty']) and 'cleaning' in task_type:
                retrieved[f"pattern_{task_type}"] = strategies

        return retrieved

    def _get_entity_info_from_kg(self, entity_name: str) -> List[str]:
        """ä»KGä¸­è·å–å®ä½“ä¿¡æ¯"""
        info = []

        if not self.scene_kg or 'nodes' not in self.scene_kg:
            return info

        # æŸ¥æ‰¾åŒ¹é…çš„èŠ‚ç‚¹
        for node in self.scene_kg['nodes']:
            node_name = node.get('name', '')
            if (node_name == entity_name or
                entity_name in node_name or
                node_name in entity_name):

                attrs = node.get('attributes', {})

                # æå–æœ‰ç”¨çš„å±æ€§ä¿¡æ¯
                if 'object_type' in attrs:
                    info.append(f"Type: {attrs['object_type']}")

                if 'openable' in attrs:
                    info.append(f"Openable: {attrs['openable']}")

                if 'receptacle' in attrs:
                    info.append(f"Can contain items: {attrs['receptacle']}")

                if 'position_x' in attrs:
                    info.append(f"Located at position ({attrs.get('position_x', 0):.1f}, {attrs.get('position_z', 0):.1f})")

                # æ·»åŠ çŠ¶æ€ä¿¡æ¯
                if node.get('type') == 'state':
                    state_value = attrs.get('state_value', '')
                    if state_value:
                        info.append(f"Current state: {state_value}")

        return info
    
    def _generate_action_with_knowledge(self, observation: Dict[str, Any], 
                                      knowledge: Dict[str, List[str]]) -> Tuple[str, str]:
        """åŸºäºæ£€ç´¢çš„çŸ¥è¯†ç”ŸæˆåŠ¨ä½œ"""
        available_actions = observation.get('available_actions', [])
        visible_entities = observation.get('visible_entities', [])
        inventory = observation.get('agent_inventory', [])
        
        # åŸºäºçŸ¥è¯†çš„å†³ç­–
        # 1. å¦‚æœçŸ¥è¯†å»ºè®®ä¼˜å…ˆæ‹¾å–é’¥åŒ™
        if any('keys' in str(k) for k in knowledge.values()) and 'pick_up' in available_actions:
            key_items = [e for e in visible_entities if 'key' in e.lower()]
            if key_items:
                return "pick_up", key_items[0]
        
        # 2. å¦‚æœæœ‰é’¥åŒ™ä¸”çŸ¥è¯†å»ºè®®è§£é”
        if any('key' in item.lower() for item in inventory) and 'open' in available_actions:
            lockable_items = [e for e in visible_entities if any(keyword in e.lower() 
                             for keyword in ['cabinet', 'drawer', 'safe'])]
            if lockable_items:
                return "open", lockable_items[0]
        
        # 3. åŸºäºä»»åŠ¡æ¨¡å¼çŸ¥è¯†
        if 'pattern_cooking_task' in knowledge and visible_entities:
            cooking_items = [e for e in visible_entities if any(keyword in e.lower() 
                            for keyword in ['microwave', 'stove', 'oven'])]
            if cooking_items and 'use' in available_actions:
                return "use", cooking_items[0]
        
        # 4. åŸºäºåŠ¨ä½œç­–ç•¥çŸ¥è¯†
        for action in available_actions:
            strategy_key = f"strategy_{action}"
            if strategy_key in knowledge:
                if action == 'examine' and visible_entities:
                    return "examine", visible_entities[0]
                elif action == 'go_to' and visible_entities:
                    return "go_to", visible_entities[0]
        
        # é»˜è®¤è¡Œä¸º
        if 'pick_up' in available_actions and visible_entities and not inventory:
            return "pick_up", visible_entities[0]
        elif 'examine' in available_actions and visible_entities:
            return "examine", visible_entities[0]
        
        return "wait", None

    def _llm_rag_generation(self, observation: Dict[str, Any], retrieved_knowledge: Dict[str, Any],
                          available_actions: List[str], visible_entities: List[str],
                          inventory: List[str]) -> Tuple[str, str]:
        """ä½¿ç”¨çœŸå®LLMè¿›è¡ŒRAGç”Ÿæˆ"""
        print(f"ğŸ”¥ Calling real LLM for RAG generation ({self.model_name})...")

        # æ„å»ºRAGæç¤º
        prompt = self._build_rag_prompt(observation, retrieved_knowledge, available_actions,
                                      visible_entities, inventory)
        print(f"ğŸ“ RAG LLM Prompt:")
        print(f"{'='*60}")
        print(prompt)
        print(f"{'='*60}")

        try:
            # è°ƒç”¨OpenAI API
            from openai import OpenAI
            from src.utils.simple_config import SimpleConfig

            config = SimpleConfig()
            api_key = config.get_api_key()
            base_url = config.get_base_url()

            client = OpenAI(
                api_key=api_key,
                base_url=base_url
            )

            response = client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are an AI agent using RAG (Retrieval-Augmented Generation). Use the retrieved knowledge to make informed decisions."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=200,
                temperature=0.3  # é™ä½éšæœºæ€§
            )

            raw_response = response.choices[0].message.content.strip()
            print(f"ğŸ¤– Raw RAG LLM Response:")
            print(f"{'='*60}")
            print(raw_response)
            print(f"{'='*60}")

            # è§£æRAGå“åº”
            action, target = self._parse_rag_response(raw_response, available_actions, visible_entities)
            print(f"âœ… Parsed RAG action: {action} -> {target}")

            return action, target

        except Exception as e:
            print(f"âŒ RAG LLM call failed: {e}")
            print("ğŸš« No fallback - experiment requires real LLM responses")
            raise e

    def _build_rag_prompt(self, observation: Dict[str, Any], retrieved_knowledge: Dict[str, Any],
                        available_actions: List[str], visible_entities: List[str],
                        inventory: List[str]) -> str:
        """æ„å»ºRAGæç¤º"""
        # æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„çŸ¥è¯†
        knowledge_text = ""
        if retrieved_knowledge:
            knowledge_text = "\nRetrieved Knowledge:\n"
            for entity, knowledge in retrieved_knowledge.items():
                if isinstance(knowledge, list):
                    knowledge_text += f"- {entity}: {', '.join(knowledge)}\n"
                else:
                    knowledge_text += f"- {entity}: {knowledge}\n"

        # è·å–å†å²åŠ¨ä½œ
        recent_actions = ""
        if self.action_history:
            recent_actions = f"\nRecent actions: {self.action_history[-3:]}"

        prompt = f"""You are using RAG (Retrieval-Augmented Generation) in a household environment.

Current situation:
- Available actions: {available_actions}
- Visible entities: {visible_entities}
- Your inventory: {inventory}{recent_actions}{knowledge_text}

TASK OBJECTIVES:
1. PRIMARY: Find and collect useful items (keys, tools, food items) - USE PICK_UP ACTION!
2. SECONDARY: Explore systematically to discover new areas and objects
3. TERTIARY: Interact with containers (cabinets, drawers) to find hidden items

CRITICAL INSTRUCTIONS:
1. **PRIORITIZE PICK_UP**: If you see any objects and have pick_up available, strongly consider picking them up
2. Use the retrieved knowledge to inform your decision and advance task objectives
3. The knowledge says "Pick up useful items first" - follow this guidance!
4. After examining objects, try to pick them up if possible
5. Collecting items gives higher rewards than just examining

Please select ONE action and ONE target from the available options.
Respond in the format: "ACTION: <action_name> TARGET: <target_name>"

For example:
- ACTION: examine TARGET: CoffeeTable_992 (to check for useful items)
- ACTION: pick_up TARGET: Key_123 (keys are high priority items)
- ACTION: go_to TARGET: Kitchen_456 (to explore new areas)

Choose wisely based on the current situation, retrieved knowledge, and task objectives:"""

        return prompt

    def _parse_rag_response(self, response: str, available_actions: List[str],
                          visible_entities: List[str]) -> Tuple[str, str]:
        """è§£æRAGå“åº” - æ”¹è¿›ç‰ˆæœ¬ï¼ŒåŒ…å«åŠ¨ä½œå…¼å®¹æ€§æ£€æŸ¥"""
        # æå–ACTIONå’ŒTARGET
        action_match = re.search(r'ACTION:\s*(\w+)', response, re.IGNORECASE)
        target_match = re.search(r'TARGET:\s*([^\s\n]+)', response, re.IGNORECASE)

        if action_match and target_match:
            action = action_match.group(1).lower()
            target = target_match.group(1)

            # éªŒè¯åŠ¨ä½œå’Œç›®æ ‡çš„åŸºæœ¬æœ‰æ•ˆæ€§
            if action in available_actions and target in visible_entities:
                # è¿›ä¸€æ­¥æ£€æŸ¥åŠ¨ä½œ-ç›®æ ‡å…¼å®¹æ€§
                if self._is_action_compatible(action, target):
                    return action, target
                else:
                    print(f"âš ï¸ Action {action} not compatible with {target}, finding alternative")

        # æ™ºèƒ½å›é€€ç­–ç•¥ - æ‰¾åˆ°å…¼å®¹çš„åŠ¨ä½œ-ç›®æ ‡ç»„åˆ
        print(f"ğŸ”„ Using intelligent fallback strategy...")
        return self._find_compatible_action(available_actions, visible_entities)

    def _is_action_compatible(self, action: str, target: str) -> bool:
        """æ£€æŸ¥åŠ¨ä½œå’Œç›®æ ‡æ˜¯å¦å…¼å®¹"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¹‹å‰å¤±è´¥çš„ç»„åˆ
        failed_key = f"{action}_{target}"
        if hasattr(self, 'failed_actions') and failed_key in self.failed_actions:
            failure_count = self.failed_actions[failed_key]
            if failure_count >= 3:  # å¦‚æœå¤±è´¥è¶…è¿‡3æ¬¡ï¼Œé¿å…é‡å¤
                print(f"âš ï¸ Avoiding repeated failure: {failed_key} (failed {failure_count} times)")
                return False

        # ä»KGè·å–ç›®æ ‡å¯¹è±¡çš„å±æ€§
        entity_info = self._get_entity_info_from_kg(target)

        # åŸºäºå¯¹è±¡å±æ€§æ£€æŸ¥åŠ¨ä½œå…¼å®¹æ€§
        if action == "pick_up":
            # æ£€æŸ¥å¯¹è±¡æ˜¯å¦å¯ä»¥æ‹¾å–
            for info in entity_info:
                if "Type:" in info:
                    object_type = info.split("Type: ")[1]
                    # å¤§å‹å®¶å…·ä¸èƒ½æ‹¾å–
                    if object_type in ["ArmChair", "CoffeeTable", "Bed", "Desk", "Sofa", "Dresser"]:
                        print(f"âš ï¸ Cannot pick up furniture: {object_type}")
                        return False
                    # å›ºå®šè£…ç½®ä¸èƒ½æ‹¾å–
                    if object_type in ["Sink", "Toilet", "Bathtub", "Stove", "Fridge"]:
                        print(f"âš ï¸ Cannot pick up fixture: {object_type}")
                        return False

        elif action == "open":
            # æ£€æŸ¥å¯¹è±¡æ˜¯å¦å¯ä»¥æ‰“å¼€
            is_openable = False
            for info in entity_info:
                if "Openable: True" in info:
                    is_openable = True
                    break
            if not is_openable:
                print(f"âš ï¸ Object {target} is not openable")
                return False

        return True

    def _find_compatible_action(self, available_actions: List[str],
                              visible_entities: List[str]) -> Tuple[str, str]:
        """æ‰¾åˆ°å…¼å®¹çš„åŠ¨ä½œ-ç›®æ ‡ç»„åˆ"""
        # ä¼˜å…ˆçº§ç­–ç•¥ï¼šexamine > go_to > pick_up > open > others
        action_priority = ["examine", "go_to", "pick_up", "open", "close", "put_down", "wait"]

        for action in action_priority:
            if action not in available_actions:
                continue

            for entity in visible_entities:
                if self._is_action_compatible(action, entity):
                    print(f"âœ… Found compatible action: {action} -> {entity}")
                    return action, entity

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…¼å®¹çš„ç»„åˆï¼Œä½¿ç”¨æœ€å®‰å…¨çš„é€‰æ‹©
        if "examine" in available_actions and visible_entities:
            print(f"ğŸ”’ Using safe fallback: examine -> {visible_entities[0]}")
            return "examine", visible_entities[0]
        elif "wait" in available_actions:
            print(f"ğŸ”’ Using wait action as last resort")
            return "wait", ""

        raise ValueError(f"Cannot find any compatible action-target combination")

    def update(self, observation: Dict[str, Any], action: str, target: str,
               reward: float, next_observation: Dict[str, Any], done: bool):
        """æ›´æ–°æ™ºèƒ½ä½“ - RAGå¯ä»¥æ›´æ–°çŸ¥è¯†åº“"""
        # åŸºäºç»éªŒæ›´æ–°çŸ¥è¯†åº“
        if reward > 0.1:  # æ­£å¥–åŠ±
            # å¼ºåŒ–æˆåŠŸçš„åŠ¨ä½œæ¨¡å¼
            success_pattern = f"{action}_{target}" if target else action
            if 'successful_patterns' not in self.knowledge_base:
                self.knowledge_base['successful_patterns'] = {}
            
            self.knowledge_base['successful_patterns'][success_pattern] = \
                self.knowledge_base['successful_patterns'].get(success_pattern, 0) + reward
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'agent_type': 'rag',
            'knowledge_base_size': len(self.knowledge_base),
            'total_retrievals': len(self.retrieval_history),
            'recent_retrievals': self.retrieval_history[-3:] if self.retrieval_history else [],
            'successful_patterns': self.knowledge_base.get('successful_patterns', {})
        }


if __name__ == "__main__":
    # æµ‹è¯•ä¸‰ä¸ªæ™ºèƒ½ä½“
    print("ğŸ§ª æµ‹è¯•åŸºçº¿æ™ºèƒ½ä½“")
    
    # æ¨¡æ‹Ÿè§‚å¯Ÿ
    mock_observation = {
        'scene': 'FloorPlan228-openable',
        'agent_location': 'Kitchen',
        'agent_inventory': [],
        'visible_entities': ['Cabinet', 'Drawer', 'Apple', 'Key'],
        'available_actions': ['go_to', 'open', 'pick_up', 'examine', 'wait'],
        'step_count': 1,
        'description': 'ä½ åœ¨å¨æˆ¿é‡Œï¼Œå¯ä»¥çœ‹åˆ°æŸœå­ã€æŠ½å±‰ã€è‹¹æœå’Œé’¥åŒ™ã€‚'
    }
    
    # æµ‹è¯•LLMåŸºçº¿
    llm_agent = LLMBaselineAgent()
    llm_action, llm_target = llm_agent.select_action(mock_observation)
    print(f"ğŸ¤– LLMåŸºçº¿: {llm_action} {llm_target or ''}")
    
    # æµ‹è¯•ReAct
    react_agent = ReActAgent()
    react_action, react_target = react_agent.select_action(mock_observation)
    print(f"ğŸ§  ReAct: {react_action} {react_target or ''}")
    
    # æµ‹è¯•RAG
    rag_agent = RAGAgent()
    rag_action, rag_target = rag_agent.select_action(mock_observation)
    print(f"ğŸ” RAG: {rag_action} {rag_target or ''}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"LLM: {llm_agent.get_statistics()}")
    print(f"ReAct: {react_agent.get_statistics()}")
    print(f"RAG: {rag_agent.get_statistics()}")
    
    print("âœ… æµ‹è¯•å®Œæˆ")
