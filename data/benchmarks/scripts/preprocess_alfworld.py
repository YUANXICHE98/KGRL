"""
ALFWorldæ•°æ®é¢„å¤„ç†è„šæœ¬

ä»ALFWorldæ•°æ®ä¸­æå–çŠ¶æ€ä¿¡æ¯ï¼Œæ„å»ºDODAFçŸ¥è¯†å›¾è°±
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from src.knowledge.dodaf_kg_builder import DODAFKGBuilder, NodeType, EdgeType


class ALFWorldPreprocessor:
    """ALFWorldæ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, data_dir: str, output_dir: str):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ¨ä½œæ˜ å°„
        self.action_mapping = {
            "go to": "ç§»åŠ¨åˆ°",
            "take": "æ‹¿å–",
            "put": "æ”¾ç½®",
            "open": "æ‰“å¼€",
            "close": "å…³é—­",
            "toggle": "åˆ‡æ¢",
            "clean": "æ¸…æ´",
            "heat": "åŠ çƒ­",
            "cool": "å†·å´",
            "use": "ä½¿ç”¨"
        }
        
        # å®ä½“ç±»å‹æ˜ å°„
        self.entity_types = {
            "receptacle": "å®¹å™¨",
            "object": "ç‰©å“",
            "appliance": "è®¾å¤‡",
            "furniture": "å®¶å…·"
        }
    
    def extract_actions_from_trajectory(self, trajectory: List[str]) -> List[Dict[str, Any]]:
        """ä»è½¨è¿¹ä¸­æå–åŠ¨ä½œåºåˆ—"""
        actions = []
        
        for step in trajectory:
            # è§£æåŠ¨ä½œ
            action_match = re.match(r'^(\w+(?:\s+\w+)*)\s+(.+)$', step.strip())
            if action_match:
                action_verb = action_match.group(1).lower()
                action_object = action_match.group(2)
                
                # æ˜ å°„åŠ¨ä½œåç§°
                mapped_action = self.action_mapping.get(action_verb, action_verb)
                
                actions.append({
                    "action": mapped_action,
                    "object": action_object,
                    "original": step.strip()
                })
        
        return actions
    
    def extract_entities_from_observation(self, observation: str) -> List[Dict[str, Any]]:
        """ä»è§‚å¯Ÿä¸­æå–å®ä½“ä¿¡æ¯"""
        entities = []
        
        # ç®€å•çš„å®ä½“æå–ï¼ˆå¯ä»¥æ”¹è¿›ï¼‰
        # æŸ¥æ‰¾å¸¸è§çš„ç‰©å“å’Œå®¹å™¨
        patterns = [
            r'(\w+(?:\s+\w+)*)\s+(?:is|are)\s+(open|closed|clean|dirty|hot|cold)',
            r'(?:a|an|the)\s+(\w+(?:\s+\w+)*)',
            r'(\w+(?:\s+\w+)*)\s+(?:on|in|under)\s+',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, observation.lower())
            for match in matches:
                if isinstance(match, tuple):
                    entity_name = match[0]
                    state = match[1] if len(match) > 1 else None
                else:
                    entity_name = match
                    state = None
                
                entities.append({
                    "name": entity_name,
                    "state": state,
                    "type": self._infer_entity_type(entity_name)
                })
        
        return entities
    
    def _infer_entity_type(self, entity_name: str) -> str:
        """æ¨æ–­å®ä½“ç±»å‹"""
        # ç®€å•çš„ç±»å‹æ¨æ–­è§„åˆ™
        containers = ["drawer", "cabinet", "fridge", "microwave", "sink", "toilet"]
        objects = ["apple", "bread", "egg", "lettuce", "tomato", "potato", "knife"]
        appliances = ["stove", "microwave", "fridge", "toaster"]
        
        entity_lower = entity_name.lower()
        
        if any(container in entity_lower for container in containers):
            return "å®¹å™¨"
        elif any(obj in entity_lower for obj in objects):
            return "ç‰©å“"
        elif any(app in entity_lower for app in appliances):
            return "è®¾å¤‡"
        else:
            return "ç‰©å“"  # é»˜è®¤ç±»å‹
    
    def build_kg_from_episode(self, episode_data: Dict[str, Any]) -> DODAFKGBuilder:
        """ä»å•ä¸ªepisodeæ„å»ºçŸ¥è¯†å›¾è°±"""
        kg_builder = DODAFKGBuilder()
        
        # æå–è½¨è¿¹ä¿¡æ¯
        if "trajectory" in episode_data:
            actions = self.extract_actions_from_trajectory(episode_data["trajectory"])
            
            # ä¸ºæ¯ä¸ªåŠ¨ä½œåˆ›å»ºèŠ‚ç‚¹å’Œå…³ç³»
            prev_state_nodes = {}
            
            for i, action_info in enumerate(actions):
                # åˆ›å»ºåŠ¨ä½œèŠ‚ç‚¹
                action_id = kg_builder.add_action_node(
                    action_info["action"],
                    {"step": i, "original_command": action_info["original"]}
                )
                
                # åˆ›å»ºæˆ–è·å–å®ä½“èŠ‚ç‚¹
                entity_name = action_info["object"]
                entity_type = self._infer_entity_type(entity_name)
                entity_id = kg_builder.add_entity_node(entity_name, entity_type)
                
                # åˆ›å»ºçŠ¶æ€è½¬æ¢
                pre_state_id = kg_builder.add_state_node(
                    f"{entity_name}_pre_{i}", "unknown"
                )
                post_state_id = kg_builder.add_state_node(
                    f"{entity_name}_post_{i}", "modified"
                )
                
                # æ·»åŠ å…³ç³»
                kg_builder.add_edge(action_id, entity_id, EdgeType.MODIFIES)
                kg_builder.add_edge(entity_id, pre_state_id, EdgeType.REQUIRES)
                kg_builder.add_edge(action_id, post_state_id, EdgeType.PRODUCES)
                kg_builder.add_edge(pre_state_id, post_state_id, EdgeType.TRANSITIONS)
                
                # å¦‚æœæœ‰å‰ä¸€ä¸ªçŠ¶æ€ï¼Œå»ºç«‹è¿æ¥
                if entity_name in prev_state_nodes:
                    kg_builder.add_edge(
                        prev_state_nodes[entity_name], 
                        pre_state_id, 
                        EdgeType.TRANSITIONS
                    )
                
                prev_state_nodes[entity_name] = post_state_id
        
        return kg_builder
    
    def process_alfworld_data(self) -> None:
        """å¤„ç†ALFWorldæ•°æ®"""
        print("ğŸš€ å¼€å§‹å¤„ç†ALFWorldæ•°æ®...")
        
        # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
        data_files = list(self.data_dir.rglob("*.json"))
        
        if not data_files:
            print("âŒ æœªæ‰¾åˆ°ALFWorldæ•°æ®æ–‡ä»¶")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        
        all_kgs = []
        
        for i, data_file in enumerate(data_files[:10]):  # å¤„ç†å‰10ä¸ªæ–‡ä»¶ä½œä¸ºç¤ºä¾‹
            print(f"ğŸ“„ å¤„ç†æ–‡ä»¶ {i+1}/{min(10, len(data_files))}: {data_file.name}")
            
            try:
                with open(data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # å¦‚æœæ˜¯episodeåˆ—è¡¨
                if isinstance(data, list):
                    for j, episode in enumerate(data[:5]):  # æ¯ä¸ªæ–‡ä»¶å¤„ç†å‰5ä¸ªepisode
                        kg = self.build_kg_from_episode(episode)
                        
                        # ä¿å­˜å•ä¸ªKG
                        output_file = self.output_dir / f"alfworld_kg_{i}_{j}.json"
                        kg.export_to_json(str(output_file))
                        all_kgs.append(kg)
                
                # å¦‚æœæ˜¯å•ä¸ªepisode
                elif isinstance(data, dict):
                    kg = self.build_kg_from_episode(data)
                    output_file = self.output_dir / f"alfworld_kg_{i}.json"
                    kg.export_to_json(str(output_file))
                    all_kgs.append(kg)
                    
            except Exception as e:
                print(f"âš ï¸  å¤„ç†æ–‡ä»¶ {data_file} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"âœ… æˆåŠŸå¤„ç† {len(all_kgs)} ä¸ªçŸ¥è¯†å›¾è°±")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self._generate_statistics_report(all_kgs)
    
    def _generate_statistics_report(self, kgs: List[DODAFKGBuilder]) -> None:
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        if not kgs:
            return
        
        total_stats = {
            "total_graphs": len(kgs),
            "total_nodes": 0,
            "total_edges": 0,
            "node_types": {},
            "edge_types": {}
        }
        
        for kg in kgs:
            stats = kg.get_statistics()
            total_stats["total_nodes"] += stats["total_nodes"]
            total_stats["total_edges"] += stats["total_edges"]
            
            # åˆå¹¶èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡
            for node_type, count in stats["node_types"].items():
                total_stats["node_types"][node_type] = \
                    total_stats["node_types"].get(node_type, 0) + count
            
            # åˆå¹¶è¾¹ç±»å‹ç»Ÿè®¡
            for edge_type, count in stats["edge_types"].items():
                total_stats["edge_types"][edge_type] = \
                    total_stats["edge_types"].get(edge_type, 0) + count
        
        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        report_file = self.output_dir / "alfworld_statistics.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(total_stats, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        print("ğŸ“ˆ ALFWorldæ•°æ®ç»Ÿè®¡:")
        for key, value in total_stats.items():
            print(f"  {key}: {value}")


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®è·¯å¾„
    alfworld_data_dir = "data/benchmarks/alfworld"
    output_dir = "data/knowledge_graphs/alfworld"
    
    # åˆ›å»ºé¢„å¤„ç†å™¨
    preprocessor = ALFWorldPreprocessor(alfworld_data_dir, output_dir)
    
    # å¤„ç†æ•°æ®
    preprocessor.process_alfworld_data()


if __name__ == "__main__":
    main()
