#!/usr/bin/env python3
"""
çŸ¥è¯†å›¾è°±æ„å»ºä¸»è„šæœ¬

ç»Ÿä¸€æ„å»ºå’Œç®¡ç†å„ç§ç±»å‹çš„çŸ¥è¯†å›¾è°±ï¼š
1. ä¸‹è½½benchmarkæ•°æ®
2. é¢„å¤„ç†æ•°æ®
3. æ„å»ºDODAFçŠ¶æ€çŸ¥è¯†å›¾è°±
4. æ„å»ºè§„åˆ™çŸ¥è¯†å›¾è°±
5. ç”Ÿæˆç»Ÿä¸€çŸ¥è¯†å›¾è°±
"""

import os
import sys
import subprocess
import argparse
from pathlib import Path
from typing import List, Dict, Any
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.knowledge.unified_kg_manager import UnifiedKGManager
from src.knowledge.dodaf_kg_builder import create_example_kg
from src.knowledge.rule_kg_builder import create_example_rule_kg


class KGBuildPipeline:
    """çŸ¥è¯†å›¾è°±æ„å»ºæµæ°´çº¿"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.project_root = Path(__file__).parent.parent
        self.data_dir = self.project_root / "data"
        self.benchmarks_dir = self.data_dir / "benchmarks"
        self.kg_dir = self.data_dir / "knowledge_graphs"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.benchmarks_dir.mkdir(parents=True, exist_ok=True)
        self.kg_dir.mkdir(parents=True, exist_ok=True)
    
    def download_benchmarks(self, datasets: List[str] = None) -> None:
        """ä¸‹è½½benchmarkæ•°æ®é›†"""
        if datasets is None:
            datasets = ["alfworld", "textworld"]
        
        scripts_dir = self.benchmarks_dir / "scripts"
        
        for dataset in datasets:
            script_name = f"download_{dataset}.sh"
            script_path = scripts_dir / script_name
            
            if script_path.exists():
                print(f"ğŸš€ ä¸‹è½½ {dataset} æ•°æ®é›†...")
                try:
                    # ä½¿è„šæœ¬å¯æ‰§è¡Œ
                    os.chmod(script_path, 0o755)
                    
                    # æ‰§è¡Œä¸‹è½½è„šæœ¬
                    result = subprocess.run(
                        [str(script_path)],
                        cwd=str(scripts_dir),
                        capture_output=True,
                        text=True,
                        timeout=3600  # 1å°æ—¶è¶…æ—¶
                    )
                    
                    if result.returncode == 0:
                        print(f"âœ… {dataset} æ•°æ®é›†ä¸‹è½½æˆåŠŸ")
                    else:
                        print(f"âŒ {dataset} æ•°æ®é›†ä¸‹è½½å¤±è´¥:")
                        print(result.stderr)
                        
                except subprocess.TimeoutExpired:
                    print(f"â° {dataset} æ•°æ®é›†ä¸‹è½½è¶…æ—¶")
                except Exception as e:
                    print(f"âš ï¸  ä¸‹è½½ {dataset} æ—¶å‡ºé”™: {e}")
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ° {dataset} ä¸‹è½½è„šæœ¬: {script_path}")
    
    def preprocess_data(self, datasets: List[str] = None) -> None:
        """é¢„å¤„ç†æ•°æ®"""
        if datasets is None:
            datasets = ["alfworld", "textworld"]
        
        scripts_dir = self.benchmarks_dir / "scripts"
        
        for dataset in datasets:
            script_name = f"preprocess_{dataset}.py"
            script_path = scripts_dir / script_name
            
            if script_path.exists():
                print(f"ğŸ”„ é¢„å¤„ç† {dataset} æ•°æ®...")
                try:
                    result = subprocess.run(
                        [sys.executable, str(script_path)],
                        cwd=str(self.project_root),
                        capture_output=True,
                        text=True,
                        timeout=1800  # 30åˆ†é’Ÿè¶…æ—¶
                    )
                    
                    if result.returncode == 0:
                        print(f"âœ… {dataset} æ•°æ®é¢„å¤„ç†æˆåŠŸ")
                        if result.stdout:
                            print(result.stdout)
                    else:
                        print(f"âŒ {dataset} æ•°æ®é¢„å¤„ç†å¤±è´¥:")
                        print(result.stderr)
                        
                except subprocess.TimeoutExpired:
                    print(f"â° {dataset} æ•°æ®é¢„å¤„ç†è¶…æ—¶")
                except Exception as e:
                    print(f"âš ï¸  é¢„å¤„ç† {dataset} æ—¶å‡ºé”™: {e}")
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ° {dataset} é¢„å¤„ç†è„šæœ¬: {script_path}")
    
    def build_dodaf_kgs(self) -> List[str]:
        """æ„å»ºDODAFçŠ¶æ€çŸ¥è¯†å›¾è°±"""
        print("ğŸ—ï¸  æ„å»ºDODAFçŠ¶æ€çŸ¥è¯†å›¾è°±...")
        
        kg_ids = []
        
        # åˆ›å»ºç¤ºä¾‹DODAFçŸ¥è¯†å›¾è°±
        example_kg = create_example_kg()
        
        # ä¿å­˜ç¤ºä¾‹å›¾è°±
        dodaf_dir = self.kg_dir / "dodaf"
        dodaf_dir.mkdir(exist_ok=True)
        
        example_kg.export_to_json(str(dodaf_dir / "example_dodaf_kg.json"))
        example_kg.export_to_graphml(str(dodaf_dir / "example_dodaf_kg.graphml"))
        
        kg_ids.append("example_dodaf")
        
        # ä»é¢„å¤„ç†çš„æ•°æ®æ„å»ºæ›´å¤šå›¾è°±
        for dataset in ["alfworld", "textworld"]:
            dataset_kg_dir = self.kg_dir / dataset
            if dataset_kg_dir.exists():
                kg_files = list(dataset_kg_dir.glob("*_kg_*.json"))
                print(f"ğŸ“Š æ‰¾åˆ° {len(kg_files)} ä¸ª {dataset} çŸ¥è¯†å›¾è°±æ–‡ä»¶")
                kg_ids.extend([f.stem for f in kg_files])
        
        print(f"âœ… æ„å»ºäº† {len(kg_ids)} ä¸ªDODAFçŸ¥è¯†å›¾è°±")
        return kg_ids
    
    def build_rule_kgs(self) -> List[str]:
        """æ„å»ºè§„åˆ™çŸ¥è¯†å›¾è°±"""
        print("ğŸ“‹ æ„å»ºè§„åˆ™çŸ¥è¯†å›¾è°±...")
        
        kg_ids = []
        
        # åˆ›å»ºç¤ºä¾‹è§„åˆ™çŸ¥è¯†å›¾è°±
        example_rule_kg = create_example_rule_kg()
        
        # ä¿å­˜ç¤ºä¾‹è§„åˆ™å›¾è°±
        rules_dir = self.kg_dir / "rules"
        rules_dir.mkdir(exist_ok=True)
        
        example_rule_kg.export_rules_to_json(str(rules_dir / "example_rules_kg.json"))
        example_rule_kg.export_to_graphml(str(rules_dir / "example_rules_kg.graphml"))
        
        kg_ids.append("example_rules")
        
        # æ„å»ºæ¸¸æˆç‰¹å®šçš„è§„åˆ™å›¾è°±
        game_rules = {
            "textworld_rules": {
                "basic_actions": {
                    "take": {"preconditions": ["ç‰©å“å¯è§", "ç‰©å“å¯æ‹¿å–"]},
                    "drop": {"preconditions": ["ç‰©å“åœ¨èƒŒåŒ…ä¸­"]},
                    "open": {"preconditions": ["å®¹å™¨å¯å¼€å¯", "å®¹å™¨æœªæ‰“å¼€"]},
                    "go": {"preconditions": ["æ–¹å‘å¯é€šè¡Œ"]}
                },
                "constraints": {
                    "è´Ÿé‡é™åˆ¶": {
                        "conditions": ["èƒŒåŒ…é‡é‡ > æœ€å¤§è´Ÿé‡"],
                        "violations": ["æ— æ³•æ‹¿å–æ›´å¤šç‰©å“"]
                    },
                    "ç©ºé—´é™åˆ¶": {
                        "conditions": ["èƒŒåŒ…ç©ºé—´å·²æ»¡"],
                        "violations": ["æ— æ³•æ”¾å…¥æ›´å¤šç‰©å“"]
                    }
                },
                "inference_rules": {
                    "ä½ç½®æ¨ç†": {
                        "premises": ["ç©å®¶åœ¨æˆ¿é—´A", "ç‰©å“åœ¨æˆ¿é—´A"],
                        "conclusions": ["ç©å®¶å¯ä»¥çœ‹åˆ°ç‰©å“"]
                    }
                }
            }
        }
        
        for rule_set_name, rules in game_rules.items():
            rule_kg = create_example_rule_kg()
            rule_kg.build_game_rules_kg(rules)
            
            rule_file = rules_dir / f"{rule_set_name}.json"
            rule_kg.export_rules_to_json(str(rule_file))
            kg_ids.append(rule_set_name)
        
        print(f"âœ… æ„å»ºäº† {len(kg_ids)} ä¸ªè§„åˆ™çŸ¥è¯†å›¾è°±")
        return kg_ids
    
    def build_unified_kg(self, dodaf_kg_ids: List[str], 
                        rule_kg_ids: List[str]) -> None:
        """æ„å»ºç»Ÿä¸€çŸ¥è¯†å›¾è°±"""
        print("ğŸ”— æ„å»ºç»Ÿä¸€çŸ¥è¯†å›¾è°±...")
        
        manager = UnifiedKGManager()
        
        # åˆ›å»ºç¤ºä¾‹ç»Ÿä¸€å›¾è°±
        example_manager = create_example_unified_kg()
        
        # å¯¼å‡ºç»Ÿä¸€å›¾è°±
        unified_dir = self.kg_dir / "unified"
        unified_dir.mkdir(exist_ok=True)
        
        example_manager.export_all_kgs(str(unified_dir))
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self._generate_unified_report(unified_dir, dodaf_kg_ids, rule_kg_ids)
        
        print("âœ… ç»Ÿä¸€çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ")
    
    def _generate_unified_report(self, output_dir: Path, 
                               dodaf_kg_ids: List[str],
                               rule_kg_ids: List[str]) -> None:
        """ç”Ÿæˆç»Ÿä¸€æŠ¥å‘Š"""
        report = {
            "build_timestamp": str(Path(__file__).stat().st_mtime),
            "dodaf_kgs": {
                "count": len(dodaf_kg_ids),
                "ids": dodaf_kg_ids
            },
            "rule_kgs": {
                "count": len(rule_kg_ids),
                "ids": rule_kg_ids
            },
            "total_kgs": len(dodaf_kg_ids) + len(rule_kg_ids),
            "output_directories": {
                "dodaf": str(self.kg_dir / "dodaf"),
                "rules": str(self.kg_dir / "rules"),
                "unified": str(output_dir)
            }
        }
        
        report_file = output_dir / "build_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š æ„å»ºæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    def run_full_pipeline(self, download: bool = True, 
                         preprocess: bool = True,
                         datasets: List[str] = None) -> None:
        """è¿è¡Œå®Œæ•´çš„æ„å»ºæµæ°´çº¿"""
        print("ğŸš€ å¼€å§‹çŸ¥è¯†å›¾è°±æ„å»ºæµæ°´çº¿...")
        
        if datasets is None:
            datasets = ["alfworld", "textworld"]
        
        try:
            # 1. ä¸‹è½½æ•°æ®
            if download:
                self.download_benchmarks(datasets)
            
            # 2. é¢„å¤„ç†æ•°æ®
            if preprocess:
                self.preprocess_data(datasets)
            
            # 3. æ„å»ºDODAFçŸ¥è¯†å›¾è°±
            dodaf_kg_ids = self.build_dodaf_kgs()
            
            # 4. æ„å»ºè§„åˆ™çŸ¥è¯†å›¾è°±
            rule_kg_ids = self.build_rule_kgs()
            
            # 5. æ„å»ºç»Ÿä¸€çŸ¥è¯†å›¾è°±
            self.build_unified_kg(dodaf_kg_ids, rule_kg_ids)
            
            print("ğŸ‰ çŸ¥è¯†å›¾è°±æ„å»ºæµæ°´çº¿å®Œæˆï¼")
            
        except Exception as e:
            print(f"âŒ æ„å»ºæµæ°´çº¿å‡ºé”™: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="çŸ¥è¯†å›¾è°±æ„å»ºå·¥å…·")
    parser.add_argument("--no-download", action="store_true", 
                       help="è·³è¿‡æ•°æ®ä¸‹è½½")
    parser.add_argument("--no-preprocess", action="store_true",
                       help="è·³è¿‡æ•°æ®é¢„å¤„ç†")
    parser.add_argument("--datasets", nargs="+", 
                       default=["alfworld", "textworld"],
                       help="è¦å¤„ç†çš„æ•°æ®é›†")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ„å»ºæµæ°´çº¿
    pipeline = KGBuildPipeline()
    
    # è¿è¡Œæµæ°´çº¿
    pipeline.run_full_pipeline(
        download=not args.no_download,
        preprocess=not args.no_preprocess,
        datasets=args.datasets
    )


if __name__ == "__main__":
    main()
