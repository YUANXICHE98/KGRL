#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆæœ¬çš„åŸºçº¿å¯¹æ¯”å®éªŒ - ä½¿ç”¨çœŸå®KGæ•°æ®å’ŒçŠ¶æ€æ›´æ–°
"""

import sys
import json
import time
import random
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.environments.scene_based_env import SceneBasedEnvironment
from src.agents.baseline_agents import LLMBaselineAgent, ReActAgent, RAGAgent
from tools.visualization.visualization import ExperimentVisualizer

class FixedBaselineExperiment:
    """ä¿®å¤ç‰ˆæœ¬çš„åŸºçº¿å®éªŒ"""
    
    def __init__(self, episodes: int = 12, max_steps: int = 15):
        self.episodes = episodes
        self.max_steps = max_steps
        
        # åˆå§‹åŒ–ç¯å¢ƒ
        self.env = SceneBasedEnvironment()
        
        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        self.agents = {
            'llm_baseline': LLMBaselineAgent(),
            'react': ReActAgent(),
            'rag': RAGAgent()
        }
        
        # ç»“æœå­˜å‚¨
        self.results = {agent_name: {
            'rewards': [],
            'steps': [],
            'success': [],
            'actions': [],
            'episode_details': []
        } for agent_name in self.agents.keys()}
        
        print(f"ğŸ¯ ä¿®å¤ç‰ˆåŸºçº¿å®éªŒ - çœŸå®KGæ•°æ®")
        print(f"ğŸ“Š å›åˆæ•°: {episodes}, æœ€å¤§æ­¥æ•°: {max_steps}")
        print(f"ğŸ¤– æ™ºèƒ½ä½“: {list(self.agents.keys())}")
        print(f"ğŸš€ å¼€å§‹ä¿®å¤ç‰ˆåŸºçº¿å®éªŒ")
        
        # æ£€æŸ¥å¯ç”¨åœºæ™¯
        if not self.env.scenes:
            raise ValueError("âŒ æ²¡æœ‰å¯ç”¨åœºæ™¯ï¼è¯·æ£€æŸ¥KGæ•°æ®")
        
        available_scenes = list(self.env.scenes.keys())
        print(f"ğŸ“Š å¯ç”¨åœºæ™¯: {len(available_scenes)}")
        print(f"ğŸ¯ åœºæ™¯åˆ—è¡¨: {available_scenes[:5]}...")
    
    def run_experiment(self) -> str:
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        start_time = time.time()
        
        # é€‰æ‹©å®éªŒåœºæ™¯
        available_scenes = list(self.env.scenes.keys())
        selected_scenes = random.sample(available_scenes, min(5, len(available_scenes)))
        print(f"ğŸ¯ é€‰æ‹©çš„åœºæ™¯: {selected_scenes}")
        
        # å¯¹æ¯ä¸ªæ™ºèƒ½ä½“è¿è¡Œå®éªŒ
        for agent_name, agent in self.agents.items():
            print(f"\nğŸ¤– æµ‹è¯•æ™ºèƒ½ä½“: {agent_name.upper()}")
            
            for episode in range(self.episodes):
                # éšæœºé€‰æ‹©åœºæ™¯
                scene_name = random.choice(selected_scenes)
                
                # è¿è¡Œå•ä¸ªå›åˆ
                episode_result = self._run_episode(agent_name, agent, scene_name, episode + 1)
                
                # è®°å½•ç»“æœ
                self.results[agent_name]['rewards'].append(episode_result['total_reward'])
                self.results[agent_name]['steps'].append(episode_result['steps'])
                self.results[agent_name]['success'].append(episode_result['success'])
                self.results[agent_name]['actions'].append(episode_result['actions'])
                self.results[agent_name]['episode_details'].append(episode_result)
                
                print(f"  å›åˆ {episode + 1}: å¥–åŠ±={episode_result['total_reward']:.3f}, "
                      f"æ­¥æ•°={episode_result['steps']}, æˆåŠŸ={episode_result['success']}")
        
        # è®¡ç®—æœ€ç»ˆåˆ†æ
        final_analysis = self._analyze_results()
        
        # ä¿å­˜ç»“æœ
        results_file = self._save_results(final_analysis, start_time)
        
        # ç”Ÿæˆå¯è§†åŒ–
        self._generate_visualizations(results_file)
        
        return results_file
    
    def _run_episode(self, agent_name: str, agent, scene_name: str, episode_num: int) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªå›åˆ"""
        # é‡ç½®ç¯å¢ƒå’Œæ™ºèƒ½ä½“
        observation = self.env.reset(scene_name)
        agent.reset()
        
        total_reward = 0
        actions_taken = []
        step_rewards = []
        
        for step in range(self.max_steps):
            # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
            action, target = agent.select_action(observation)
            actions_taken.append({'action': action, 'target': target})
            
            # ç¯å¢ƒæ‰§è¡ŒåŠ¨ä½œ
            observation, reward, done, info = self.env.step(action, target)
            
            total_reward += reward
            step_rewards.append(reward)
            
            if done:
                break
        
        return {
            'agent': agent_name,
            'scene': scene_name,
            'episode': episode_num,
            'total_reward': total_reward,
            'steps': step + 1,
            'success': done and info.get('task_completed', False),
            'actions': actions_taken,
            'step_rewards': step_rewards,
            'final_info': info
        }
    
    def _analyze_results(self) -> Dict[str, Any]:
        """åˆ†æå®éªŒç»“æœ"""
        analysis = {}
        
        for agent_name, results in self.results.items():
            rewards = results['rewards']
            steps = results['steps']
            success = results['success']
            
            analysis[agent_name] = {
                'avg_reward': sum(rewards) / len(rewards) if rewards else 0,
                'std_reward': self._calculate_std(rewards),
                'success_rate': sum(success) / len(success) if success else 0,
                'avg_steps': sum(steps) / len(steps) if steps else 0,
                'best_reward': max(rewards) if rewards else 0,
                'worst_reward': min(rewards) if rewards else 0,
                'total_episodes': len(rewards)
            }
        
        return analysis
    
    def _calculate_std(self, values: List[float]) -> float:
        """è®¡ç®—æ ‡å‡†å·®"""
        if len(values) < 2:
            return 0.0
        
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance ** 0.5
    
    def _save_results(self, final_analysis: Dict[str, Any], start_time: float) -> str:
        """ä¿å­˜å®éªŒç»“æœ"""
        timestamp = int(time.time())
        
        results_data = {
            'experiment_info': {
                'type': 'fixed_baseline_experiment',
                'timestamp': timestamp,
                'duration': time.time() - start_time,
                'episodes': self.episodes,
                'max_steps': self.max_steps,
                'agents': list(self.agents.keys()),
                'scenes_used': len(self.env.scenes)
            },
            'results': self.results,
            'final_analysis': final_analysis
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        results_dir = Path("experiments/results/baseline_comparison")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        results_file = results_dir / f"fixed_baseline_experiment_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        return str(results_file)
    
    def _generate_visualizations(self, results_file: str):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        print(f"ğŸ“Š ç”Ÿæˆå®éªŒå¯è§†åŒ–...")
        
        try:
            visualizer = ExperimentVisualizer()
            
            # ç”ŸæˆåŸºçº¿å¯¹æ¯”å›¾
            visualizer.plot_baseline_comparison(results_file)
            
            # ç”Ÿæˆå­¦ä¹ æ›²çº¿
            visualizer.plot_learning_curves(results_file)
            
            # ç”Ÿæˆæ€§èƒ½é›·è¾¾å›¾
            visualizer.plot_performance_radar(results_file)
            
            print(f"ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
    
    def print_summary(self, final_analysis: Dict[str, Any]):
        """æ‰“å°å®éªŒæ€»ç»“"""
        print(f"\nğŸ¯ ä¿®å¤ç‰ˆå®éªŒæ€»ç»“:")
        print("=" * 60)
        
        # æ€§èƒ½æ’å
        agents_by_reward = sorted(final_analysis.items(), 
                                key=lambda x: x[1]['avg_reward'], reverse=True)
        
        print(f"ğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
        for i, (agent, stats) in enumerate(agents_by_reward, 1):
            print(f"{i}. {agent.upper()}:")
            print(f"   - å¹³å‡å¥–åŠ±: {stats['avg_reward']:.3f} Â± {stats['std_reward']:.3f}")
            print(f"   - æˆåŠŸç‡: {stats['success_rate']:.3f}")
            print(f"   - å¹³å‡æ­¥æ•°: {stats['avg_steps']:.1f}")
            print(f"   - æœ€ä½³å¥–åŠ±: {stats['best_reward']:.3f}")
        
        print(f"\nğŸ† æ€§èƒ½æ’å:")
        ranking = " > ".join([f"{agent}({stats['avg_reward']:.3f})" 
                            for agent, stats in agents_by_reward])
        print(f"æŒ‰å¹³å‡å¥–åŠ±: {ranking}")

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºå®éªŒ
    experiment = FixedBaselineExperiment(episodes=12, max_steps=15)
    
    # è¿è¡Œå®éªŒ
    results_file = experiment.run_experiment()
    
    # æ‰“å°æ€»ç»“
    with open(results_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    experiment.print_summary(data['final_analysis'])
    
    print(f"\nğŸ‰ ä¿®å¤ç‰ˆå®éªŒå®Œæˆ! ç»“æœ: {results_file}")

if __name__ == "__main__":
    main()
