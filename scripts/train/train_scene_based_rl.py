#!/usr/bin/env python3
"""
åŸºäºåœºæ™¯çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒè„šæœ¬
Scene-based Reinforcement Learning Training Script
"""

import json
import time
import random
from pathlib import Path
from typing import Dict, Any, List
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.environments.scene_based_env import SceneBasedEnvironment
from src.agents.rl_kg_agent import RLKGAgent
from src.utils.config_manager import get_config


class SceneBasedTrainer:
    """åŸºäºåœºæ™¯çš„RLè®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str = None):
        self.config = get_config('agents') if config_path is None else self._load_config(config_path)
        
        # åˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“
        self.env = SceneBasedEnvironment()
        self.agent = RLKGAgent(use_kg=True)
        self.baseline_agent = RLKGAgent(use_kg=False)  # å¯¹æ¯”åŸºçº¿
        
        # è®­ç»ƒè®¾ç½®
        self.episodes = self.config.get('training', {}).get('episodes', 100)
        self.max_steps = self.config.get('training', {}).get('max_steps_per_episode', 50)
        self.eval_frequency = self.config.get('training', {}).get('evaluation', {}).get('frequency', 20)
        
        # ç»“æœè®°å½•
        self.training_results = {
            'kg_agent': {'rewards': [], 'success_rates': [], 'steps': []},
            'baseline_agent': {'rewards': [], 'success_rates': [], 'steps': []}
        }
        
        print(f"ğŸ‹ï¸ åˆå§‹åŒ–åœºæ™¯è®­ç»ƒå™¨ (episodes: {self.episodes})")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            import yaml
            return yaml.safe_load(f)
    
    def train_single_episode(self, agent, scene_name: str = None) -> Dict[str, Any]:
        """è®­ç»ƒå•ä¸ªepisode"""
        # é‡ç½®ç¯å¢ƒ
        obs = self.env.reset(scene_name)
        agent.reset(obs)
        
        total_reward = 0
        steps = 0
        done = False
        
        while not done and steps < self.max_steps:
            # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
            action, target = agent.select_action(obs)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, done, info = self.env.step(action, target)
            
            # æ™ºèƒ½ä½“å­¦ä¹ 
            agent.update(obs, action, target, reward, next_obs, done)
            
            # æ›´æ–°çŠ¶æ€
            obs = next_obs
            total_reward += reward
            steps += 1
        
        # è®¡ç®—æˆåŠŸç‡
        success = info.get('task_completed', False) or total_reward > 5
        
        return {
            'total_reward': total_reward,
            'steps': steps,
            'success': success,
            'scene': obs['scene']
        }
    
    def evaluate_agent(self, agent, num_episodes: int = 10) -> Dict[str, float]:
        """è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½"""
        results = []
        
        for _ in range(num_episodes):
            result = self.train_single_episode(agent)
            results.append(result)
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        total_rewards = [r['total_reward'] for r in results]
        success_count = sum(1 for r in results if r['success'])
        total_steps = [r['steps'] for r in results]
        
        return {
            'avg_reward': sum(total_rewards) / len(total_rewards),
            'success_rate': success_count / len(results),
            'avg_steps': sum(total_steps) / len(total_steps),
            'episodes': len(results)
        }
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {self.episodes} episodes")
        
        # è·å–å¯ç”¨åœºæ™¯
        scenes = self.env.get_scene_list()
        print(f"ğŸ“Š å¯ç”¨åœºæ™¯: {len(scenes)} ä¸ª")
        
        if not scenes:
            print("âŒ æ²¡æœ‰å¯ç”¨åœºæ™¯ï¼Œè¯·å…ˆæ„å»ºçŸ¥è¯†å›¾è°±")
            return
        
        start_time = time.time()
        
        for episode in range(self.episodes):
            # éšæœºé€‰æ‹©åœºæ™¯
            scene_name = random.choice(scenes)
            
            # è®­ç»ƒKGå¢å¼ºæ™ºèƒ½ä½“
            kg_result = self.train_single_episode(self.agent, scene_name)
            
            # è®­ç»ƒåŸºçº¿æ™ºèƒ½ä½“
            baseline_result = self.train_single_episode(self.baseline_agent, scene_name)
            
            # è®°å½•ç»“æœ
            self.training_results['kg_agent']['rewards'].append(kg_result['total_reward'])
            self.training_results['kg_agent']['success_rates'].append(1 if kg_result['success'] else 0)
            self.training_results['kg_agent']['steps'].append(kg_result['steps'])
            
            self.training_results['baseline_agent']['rewards'].append(baseline_result['total_reward'])
            self.training_results['baseline_agent']['success_rates'].append(1 if baseline_result['success'] else 0)
            self.training_results['baseline_agent']['steps'].append(baseline_result['steps'])
            
            # å®šæœŸè¯„ä¼°å’ŒæŠ¥å‘Š
            if (episode + 1) % self.eval_frequency == 0:
                self._report_progress(episode + 1)
        
        # æœ€ç»ˆè¯„ä¼°
        print(f"\nğŸ¯ æœ€ç»ˆè¯„ä¼°:")
        self._final_evaluation()
        
        # ä¿å­˜ç»“æœ
        self._save_results()
        
        training_time = time.time() - start_time
        print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
    
    def _report_progress(self, episode: int):
        """æŠ¥å‘Šè®­ç»ƒè¿›åº¦"""
        print(f"\nğŸ“Š Episode {episode} è¿›åº¦æŠ¥å‘Š:")
        
        # è®¡ç®—æœ€è¿‘çš„æ€§èƒ½
        recent_episodes = min(self.eval_frequency, len(self.training_results['kg_agent']['rewards']))
        
        # KGæ™ºèƒ½ä½“æ€§èƒ½
        kg_recent_rewards = self.training_results['kg_agent']['rewards'][-recent_episodes:]
        kg_recent_success = self.training_results['kg_agent']['success_rates'][-recent_episodes:]
        kg_recent_steps = self.training_results['kg_agent']['steps'][-recent_episodes:]
        
        kg_avg_reward = sum(kg_recent_rewards) / len(kg_recent_rewards)
        kg_success_rate = sum(kg_recent_success) / len(kg_recent_success)
        kg_avg_steps = sum(kg_recent_steps) / len(kg_recent_steps)
        
        # åŸºçº¿æ™ºèƒ½ä½“æ€§èƒ½
        baseline_recent_rewards = self.training_results['baseline_agent']['rewards'][-recent_episodes:]
        baseline_recent_success = self.training_results['baseline_agent']['success_rates'][-recent_episodes:]
        baseline_recent_steps = self.training_results['baseline_agent']['steps'][-recent_episodes:]
        
        baseline_avg_reward = sum(baseline_recent_rewards) / len(baseline_recent_rewards)
        baseline_success_rate = sum(baseline_recent_success) / len(baseline_recent_success)
        baseline_avg_steps = sum(baseline_recent_steps) / len(baseline_recent_steps)
        
        print(f"ğŸ§  KGæ™ºèƒ½ä½“ - å¹³å‡å¥–åŠ±: {kg_avg_reward:.3f}, æˆåŠŸç‡: {kg_success_rate:.3f}, å¹³å‡æ­¥æ•°: {kg_avg_steps:.1f}")
        print(f"ğŸ“‹ åŸºçº¿æ™ºèƒ½ä½“ - å¹³å‡å¥–åŠ±: {baseline_avg_reward:.3f}, æˆåŠŸç‡: {baseline_success_rate:.3f}, å¹³å‡æ­¥æ•°: {baseline_avg_steps:.1f}")
        
        # æ€§èƒ½å¯¹æ¯”
        reward_improvement = ((kg_avg_reward - baseline_avg_reward) / max(abs(baseline_avg_reward), 0.001)) * 100
        success_improvement = (kg_success_rate - baseline_success_rate) * 100
        
        print(f"ğŸ“ˆ KGå¢å¼ºæ•ˆæœ - å¥–åŠ±æå‡: {reward_improvement:+.1f}%, æˆåŠŸç‡æå‡: {success_improvement:+.1f}%")
    
    def _final_evaluation(self):
        """æœ€ç»ˆè¯„ä¼°"""
        print("ğŸ” è¿›è¡Œæœ€ç»ˆè¯¦ç»†è¯„ä¼°...")
        
        # è¯¦ç»†è¯„ä¼°ä¸¤ä¸ªæ™ºèƒ½ä½“
        kg_eval = self.evaluate_agent(self.agent, num_episodes=20)
        baseline_eval = self.evaluate_agent(self.baseline_agent, num_episodes=20)
        
        print(f"\nğŸ§  KGå¢å¼ºæ™ºèƒ½ä½“æœ€ç»ˆæ€§èƒ½:")
        print(f"   - å¹³å‡å¥–åŠ±: {kg_eval['avg_reward']:.3f}")
        print(f"   - æˆåŠŸç‡: {kg_eval['success_rate']:.3f}")
        print(f"   - å¹³å‡æ­¥æ•°: {kg_eval['avg_steps']:.1f}")
        
        print(f"\nğŸ“‹ åŸºçº¿æ™ºèƒ½ä½“æœ€ç»ˆæ€§èƒ½:")
        print(f"   - å¹³å‡å¥–åŠ±: {baseline_eval['avg_reward']:.3f}")
        print(f"   - æˆåŠŸç‡: {baseline_eval['success_rate']:.3f}")
        print(f"   - å¹³å‡æ­¥æ•°: {baseline_eval['avg_steps']:.1f}")
        
        # è®¡ç®—æ”¹è¿›
        reward_improvement = ((kg_eval['avg_reward'] - baseline_eval['avg_reward']) / max(abs(baseline_eval['avg_reward']), 0.001)) * 100
        success_improvement = (kg_eval['success_rate'] - baseline_eval['success_rate']) * 100
        step_efficiency = ((baseline_eval['avg_steps'] - kg_eval['avg_steps']) / max(baseline_eval['avg_steps'], 1)) * 100
        
        print(f"\nğŸ¯ KGå¢å¼ºæ€»ä½“æ•ˆæœ:")
        print(f"   - å¥–åŠ±æ”¹è¿›: {reward_improvement:+.1f}%")
        print(f"   - æˆåŠŸç‡æ”¹è¿›: {success_improvement:+.1f}%")
        print(f"   - æ­¥æ•°æ•ˆç‡æ”¹è¿›: {step_efficiency:+.1f}%")
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        self.final_evaluation = {
            'kg_agent': kg_eval,
            'baseline_agent': baseline_eval,
            'improvements': {
                'reward': reward_improvement,
                'success_rate': success_improvement,
                'step_efficiency': step_efficiency
            }
        }
    
    def _save_results(self):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        results_dir = Path("results/training")
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è®­ç»ƒå†å²
        training_file = results_dir / f"scene_training_{int(time.time())}.json"
        
        results_data = {
            'config': {
                'episodes': self.episodes,
                'max_steps': self.max_steps,
                'eval_frequency': self.eval_frequency
            },
            'training_results': self.training_results,
            'final_evaluation': getattr(self, 'final_evaluation', {}),
            'agent_stats': {
                'kg_agent': self.agent.get_statistics(),
                'baseline_agent': self.baseline_agent.get_statistics()
            },
            'scenes_used': self.env.get_scene_list()
        }
        
        with open(training_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ è®­ç»ƒç»“æœä¿å­˜åˆ°: {training_file}")
        
        # ä¿å­˜æ™ºèƒ½ä½“çŠ¶æ€
        agent_dir = Path("checkpoints/agents")
        agent_dir.mkdir(parents=True, exist_ok=True)
        
        kg_agent_file = agent_dir / f"kg_agent_{int(time.time())}.json"
        baseline_agent_file = agent_dir / f"baseline_agent_{int(time.time())}.json"
        
        self.agent.save_agent(str(kg_agent_file))
        self.baseline_agent.save_agent(str(baseline_agent_file))
    
    def create_training_report(self) -> str:
        """åˆ›å»ºè®­ç»ƒæŠ¥å‘Š"""
        if not hasattr(self, 'final_evaluation'):
            return "è®­ç»ƒå°šæœªå®Œæˆï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š"
        
        report = f"""
# åœºæ™¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒæŠ¥å‘Š

## è®­ç»ƒé…ç½®
- æ€»Episodes: {self.episodes}
- æœ€å¤§æ­¥æ•°/Episode: {self.max_steps}
- è¯„ä¼°é¢‘ç‡: {self.eval_frequency}
- åœºæ™¯æ•°é‡: {len(self.env.get_scene_list())}

## æœ€ç»ˆæ€§èƒ½å¯¹æ¯”

### KGå¢å¼ºæ™ºèƒ½ä½“
- å¹³å‡å¥–åŠ±: {self.final_evaluation['kg_agent']['avg_reward']:.3f}
- æˆåŠŸç‡: {self.final_evaluation['kg_agent']['success_rate']:.3f}
- å¹³å‡æ­¥æ•°: {self.final_evaluation['kg_agent']['avg_steps']:.1f}

### åŸºçº¿æ™ºèƒ½ä½“
- å¹³å‡å¥–åŠ±: {self.final_evaluation['baseline_agent']['avg_reward']:.3f}
- æˆåŠŸç‡: {self.final_evaluation['baseline_agent']['success_rate']:.3f}
- å¹³å‡æ­¥æ•°: {self.final_evaluation['baseline_agent']['avg_steps']:.1f}

## KGå¢å¼ºæ•ˆæœ
- å¥–åŠ±æ”¹è¿›: {self.final_evaluation['improvements']['reward']:+.1f}%
- æˆåŠŸç‡æ”¹è¿›: {self.final_evaluation['improvements']['success_rate']:+.1f}%
- æ­¥æ•°æ•ˆç‡æ”¹è¿›: {self.final_evaluation['improvements']['step_efficiency']:+.1f}%

## ç»“è®º
{'KGå¢å¼ºæ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“æ€§èƒ½' if self.final_evaluation['improvements']['reward'] > 5 else 'KGå¢å¼ºæ•ˆæœæœ‰é™ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–'}
        """
        
        return report.strip()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ åœºæ™¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SceneBasedTrainer()
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = trainer.create_training_report()
    print(f"\nğŸ“‹ è®­ç»ƒæŠ¥å‘Š:\n{report}")
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    main()
