"""
æµ‹è¯•è®°å¿†å¢å¼ºæ™ºèƒ½ä½“
éªŒè¯è®°å¿†ç³»ç»Ÿä¸æ™ºèƒ½ä½“çš„é›†æˆæ•ˆæœ
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.agents.memory_agent import MemoryAgent
from src.knowledge.kg_builder import KnowledgeGraphBuilder
from src.knowledge.kg_retriever import KnowledgeGraphRetriever
from src.memory.medium_term_memory import MediumTermMemory, ActionSequence
from src.memory.long_term_memory import LongTermMemory, DecisionPattern, TaskStrategy
from src.memory.short_term_memory import ActionMemory
import time


def test_memory_enhanced_agent_creation():
    """æµ‹è¯•è®°å¿†å¢å¼ºAgentçš„åˆ›å»º"""
    print("ğŸ¤– æµ‹è¯•è®°å¿†å¢å¼ºAgentåˆ›å»º...")
    
    config = {
        'model_name': 'gpt-4o',
        'use_local_model': False,
        'use_memory': True,
        'memory_max_actions': 5,
        'use_knowledge_graph': True,
        'max_kg_facts': 3,
        'prevent_loops': True,
        'temperature': 0.7,
        'max_tokens': 100
    }
    
    agent = MemoryEnhancedRAGAgent('test_memory_agent', config)
    
    print(f"âœ… Agentåˆ›å»ºæˆåŠŸ: {agent.agent_id}")
    print(f"âœ… è®°å¿†ç³»ç»Ÿå¯ç”¨: {agent.use_memory}")
    print(f"âœ… çŸ¥è¯†å›¾è°±å¯ç”¨: {agent.use_knowledge_graph}")
    print(f"âœ… å¾ªç¯é˜²æ­¢å¯ç”¨: {agent.prevent_loops}")
    
    return agent


def test_knowledge_integration():
    """æµ‹è¯•çŸ¥è¯†å›¾è°±é›†æˆ"""
    print("\nğŸ“š æµ‹è¯•çŸ¥è¯†å›¾è°±é›†æˆ...")
    
    # åˆ›å»ºçŸ¥è¯†å›¾è°±
    kg = KnowledgeGraphBuilder('test_kg')
    kg.add_fact('kitchen', 'contains', 'key')
    kg.add_fact('key', 'opens', 'chest')
    kg.add_fact('chest', 'location', 'bedroom')
    kg.add_fact('bedroom', 'connected_to', 'living_room')
    
    retriever = KnowledgeGraphRetriever(kg, 'test_retriever')
    
    # åˆ›å»ºAgentå¹¶è®¾ç½®çŸ¥è¯†æ£€ç´¢å™¨
    agent = MemoryEnhancedRAGAgent('kg_test_agent', {
        'use_memory': True,
        'use_knowledge_graph': True,
        'max_kg_facts': 3
    })
    
    agent.set_knowledge_retriever(retriever)
    
    print(f"âœ… çŸ¥è¯†å›¾è°±åˆ›å»º: {len(kg.facts)} ä¸ªäº‹å®")
    print("âœ… çŸ¥è¯†æ£€ç´¢å™¨è®¾ç½®æˆåŠŸ")
    
    return agent, retriever


def test_memory_guided_decision():
    """æµ‹è¯•è®°å¿†å¼•å¯¼çš„å†³ç­–"""
    print("\nğŸ§  æµ‹è¯•è®°å¿†å¼•å¯¼çš„å†³ç­–...")
    
    # åˆ›å»ºAgentå’ŒçŸ¥è¯†å›¾è°±
    agent, retriever = test_knowledge_integration()
    
    # æ¨¡æ‹Ÿä¸€ç³»åˆ—å†³ç­–åœºæ™¯
    scenarios = [
        {
            "observation": "You are in a kitchen. There is a key on the table.",
            "available_actions": ["take key", "go north", "look"],
            "expected_memory": "åº”è¯¥è®°ä½åœ¨å¨æˆ¿çœ‹åˆ°äº†é’¥åŒ™"
        },
        {
            "observation": "You are in a kitchen. You are carrying a key.",
            "available_actions": ["go north", "drop key", "look"],
            "expected_memory": "åº”è¯¥è®°ä½å·²ç»æ‹¿åˆ°äº†é’¥åŒ™"
        },
        {
            "observation": "You are in a living room. There is a sofa here.",
            "available_actions": ["go east", "go south", "look"],
            "expected_memory": "åº”è¯¥è®°ä½ä»å¨æˆ¿ç§»åŠ¨åˆ°äº†å®¢å…"
        }
    ]
    
    previous_observation = ""
    
    for i, scenario in enumerate(scenarios):
        print(f"\n--- åœºæ™¯ {i+1} ---")
        print(f"è§‚å¯Ÿ: {scenario['observation']}")
        print(f"å¯ç”¨åŠ¨ä½œ: {scenario['available_actions']}")
        
        # æ‰§è¡Œå†³ç­– (ä¸è°ƒç”¨çœŸå®API)
        try:
            # æ¨¡æ‹Ÿå†³ç­–è¿‡ç¨‹ (ä¸å®é™…è°ƒç”¨LLM)
            action = agent._choose_alternative_action(
                scenario['available_actions'], 
                agent._get_memory_context()
            )
            print(f"é€‰æ‹©åŠ¨ä½œ: {action}")
            
            # è®°å½•åŠ¨ä½œç»“æœåˆ°è®°å¿†
            if previous_observation:
                agent.record_action_result(
                    action="simulated_action",
                    observation_before=previous_observation,
                    observation_after=scenario['observation'],
                    reward=0.1,
                    done=False
                )
            
            previous_observation = scenario['observation']
            
        except Exception as e:
            print(f"å†³ç­–è¿‡ç¨‹å‡ºé”™: {e}")
    
    # æ£€æŸ¥è®°å¿†çŠ¶æ€
    if agent.short_term_memory:
        stats = agent.short_term_memory.get_stats()
        print(f"\nâœ… è®°å¿†ç»Ÿè®¡: {stats}")
        
        context = agent._get_memory_context()
        print(f"âœ… å†³ç­–ä¸Šä¸‹æ–‡åŒ…å« {len(context)} ä¸ªå­—æ®µ")


def test_loop_detection():
    """æµ‹è¯•å¾ªç¯æ£€æµ‹åŠŸèƒ½"""
    print("\nğŸ”„ æµ‹è¯•å¾ªç¯æ£€æµ‹åŠŸèƒ½...")
    
    agent = MemoryEnhancedRAGAgent('loop_test_agent', {
        'use_memory': True,
        'prevent_loops': True,
        'loop_detection_window': 1
    })
    
    # æ¨¡æ‹Ÿé‡å¤åŠ¨ä½œ
    agent.record_action_result(
        action="look",
        observation_before="You are in a room.",
        observation_after="You see nothing new.",
        reward=0.0,
        done=False
    )
    
    time.sleep(0.1)
    
    agent.record_action_result(
        action="look",
        observation_before="You are in a room.",
        observation_after="You see nothing new.",
        reward=0.0,
        done=False
    )
    
    # æµ‹è¯•å¾ªç¯æ£€æµ‹
    available_actions = ["look", "go north", "take item"]
    is_loop = agent._detect_potential_loop(available_actions)
    
    print(f"âœ… å¾ªç¯æ£€æµ‹ç»“æœ: {is_loop}")
    
    if is_loop:
        alternative_action = agent._choose_alternative_action(
            available_actions, 
            agent._get_memory_context()
        )
        print(f"âœ… é€‰æ‹©æ›¿ä»£åŠ¨ä½œ: {alternative_action}")


def test_memory_keyword_extraction():
    """æµ‹è¯•è®°å¿†å…³é”®è¯æå–"""
    print("\nğŸ” æµ‹è¯•è®°å¿†å…³é”®è¯æå–...")
    
    agent = MemoryEnhancedRAGAgent('keyword_test_agent', {
        'use_memory': True,
        'use_knowledge_graph': True
    })
    
    # æ·»åŠ ä¸€äº›è®°å¿†æ•°æ®
    agent.record_action_result(
        action="take key",
        observation_before="You are in a kitchen.",
        observation_after="You are carrying a key.",
        reward=0.5,
        done=False
    )
    
    agent.record_action_result(
        action="go north",
        observation_before="You are in a kitchen.",
        observation_after="You are in a living room.",
        reward=0.1,
        done=False
    )
    
    # è·å–è®°å¿†ä¸Šä¸‹æ–‡å¹¶æå–å…³é”®è¯
    memory_context = agent._get_memory_context()
    keywords = agent._extract_memory_keywords(memory_context)
    
    print(f"âœ… æå–çš„è®°å¿†å…³é”®è¯: {keywords}")
    print(f"âœ… å…³é”®è¯æ•°é‡: {len(keywords)}")


def test_agent_stats():
    """æµ‹è¯•Agentç»Ÿè®¡åŠŸèƒ½"""
    print("\nğŸ“Š æµ‹è¯•Agentç»Ÿè®¡åŠŸèƒ½...")
    
    agent = MemoryEnhancedRAGAgent('stats_test_agent', {
        'use_memory': True,
        'use_knowledge_graph': True
    })
    
    # æ¨¡æ‹Ÿä¸€äº›æ´»åŠ¨
    agent.total_actions = 5
    agent.memory_guided_decisions = 3
    agent.loop_preventions = 1
    agent.api_calls = 2
    agent.api_response_times = [1.2, 1.5]
    
    # æ·»åŠ è®°å¿†æ•°æ®
    agent.record_action_result("test", "before", "after", 0.1, False)
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = agent.get_stats()
    
    print("âœ… Agentç»Ÿè®¡ä¿¡æ¯:")
    for key, value in stats.items():
        print(f"  - {key}: {value}")


def test_agent_reset():
    """æµ‹è¯•Agenté‡ç½®åŠŸèƒ½"""
    print("\nğŸ”„ æµ‹è¯•Agenté‡ç½®åŠŸèƒ½...")
    
    agent = MemoryEnhancedRAGAgent('reset_test_agent', {
        'use_memory': True
    })
    
    # æ·»åŠ ä¸€äº›æ•°æ®
    agent.total_actions = 10
    agent.record_action_result("test", "before", "after", 0.1, False)
    
    print(f"é‡ç½®å‰ - æ€»åŠ¨ä½œæ•°: {agent.total_actions}")
    if agent.short_term_memory:
        print(f"é‡ç½®å‰ - è®°å¿†ä¸­åŠ¨ä½œæ•°: {agent.short_term_memory.get_stats()['action_count']}")
    
    # é‡ç½®
    agent.reset()
    
    print(f"é‡ç½®å - æ€»åŠ¨ä½œæ•°: {agent.total_actions}")
    if agent.short_term_memory:
        print(f"é‡ç½®å - è®°å¿†ä¸­åŠ¨ä½œæ•°: {agent.short_term_memory.get_stats()['action_count']}")
    
    print("âœ… Agenté‡ç½®æˆåŠŸ")


def test_medium_term_memory():
    """æµ‹è¯•ä¸­æœŸè®°å¿†ç³»ç»Ÿ"""
    print("\nğŸ§  æµ‹è¯•ä¸­æœŸè®°å¿†ç³»ç»Ÿ...")

    memory = MediumTermMemory("test_medium", max_sequences=10)

    # æµ‹è¯•åŠ¨ä½œåºåˆ—è®°å½•
    memory.start_new_sequence("find treasure")

    # æ¨¡æ‹Ÿä¸€ä¸ªæˆåŠŸçš„åŠ¨ä½œåºåˆ—
    actions = [
        ActionMemory("look", "You are in kitchen", "You see a key", 0.0, False, time.time()),
        ActionMemory("take key", "You see a key", "You have key", 0.5, True, time.time() + 1),
        ActionMemory("go north", "You have key", "You are in living room", 0.1, True, time.time() + 2),
        ActionMemory("go east", "You are in living room", "You are in bedroom", 0.1, True, time.time() + 3),
        ActionMemory("open chest", "You are in bedroom", "Chest opened, treasure found!", 1.0, True, time.time() + 4)
    ]

    for action in actions:
        memory.add_action_to_sequence(action)

    # ç»“æŸåºåˆ—
    sequence = memory.end_current_sequence(success=True, reason="goal_achieved")

    print(f"âœ… æˆåŠŸåºåˆ—è®°å½•: {sequence.sequence_id}")
    print(f"âœ… åºåˆ—é•¿åº¦: {len(sequence.actions)}")
    print(f"âœ… æ€»å¥–åŠ±: {sequence.total_reward}")

    # æµ‹è¯•å¤±è´¥åºåˆ—
    memory.start_new_sequence("find treasure again")

    failed_actions = [
        ActionMemory("look", "You are in kitchen", "You see nothing", 0.0, False, time.time()),
        ActionMemory("look", "You see nothing", "Still nothing", 0.0, False, time.time() + 1),
        ActionMemory("look", "Still nothing", "Nothing again", -0.1, False, time.time() + 2)
    ]

    for action in failed_actions:
        memory.add_action_to_sequence(action)

    failed_sequence = memory.end_current_sequence(success=False, reason="timeout")

    print(f"âœ… å¤±è´¥åºåˆ—è®°å½•: {failed_sequence.sequence_id}")

    # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
    stats = memory.get_stats()
    print(f"âœ… è®°å¿†ç»Ÿè®¡: {stats}")

    # æµ‹è¯•åŠ¨ä½œç»Ÿè®¡
    action_stats = memory.get_action_statistics("look")
    print(f"âœ… 'look'åŠ¨ä½œç»Ÿè®¡: {action_stats}")

    # æµ‹è¯•æ¨èåŠ¨ä½œ
    recommendations = memory.get_recommended_actions()
    print(f"âœ… æ¨èåŠ¨ä½œ: {recommendations}")

    return memory


def test_pattern_analysis():
    """æµ‹è¯•æ¨¡å¼åˆ†æåŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•æ¨¡å¼åˆ†æåŠŸèƒ½...")

    memory = MediumTermMemory("pattern_test")

    # åˆ›å»ºå¤šä¸ªç›¸ä¼¼çš„æˆåŠŸåºåˆ—
    for i in range(3):
        memory.start_new_sequence(f"treasure_hunt_{i}")

        # ç›¸åŒçš„æˆåŠŸæ¨¡å¼
        success_actions = [
            ActionMemory("take key", "see key", "have key", 0.5, True, time.time()),
            ActionMemory("go north", "have key", "in living room", 0.1, True, time.time() + 1),
            ActionMemory("go east", "in living room", "in bedroom", 0.1, True, time.time() + 2),
            ActionMemory("open chest", "in bedroom", "treasure found", 1.0, True, time.time() + 3)
        ]

        for action in success_actions:
            memory.add_action_to_sequence(action)

        memory.end_current_sequence(success=True)
        time.sleep(0.1)

    # åˆ›å»ºå¤±è´¥åºåˆ—
    for i in range(2):
        memory.start_new_sequence(f"failed_attempt_{i}")

        fail_actions = [
            ActionMemory("look", "in room", "see nothing", 0.0, False, time.time()),
            ActionMemory("look", "see nothing", "still nothing", -0.1, False, time.time() + 1)
        ]

        for action in fail_actions:
            memory.add_action_to_sequence(action)

        memory.end_current_sequence(success=False)
        time.sleep(0.1)

    # è·å–æ¨¡å¼åˆ†æ
    success_patterns = memory.get_success_patterns()
    failure_patterns = memory.get_failure_patterns()

    print(f"âœ… æˆåŠŸæ¨¡å¼æ•°é‡: {len(success_patterns)}")
    for pattern in success_patterns:
        print(f"  - {pattern.pattern_description} (é¢‘ç‡: {pattern.frequency})")

    print(f"âœ… å¤±è´¥æ¨¡å¼æ•°é‡: {len(failure_patterns)}")
    for pattern in failure_patterns:
        print(f"  - {pattern.pattern_description} (é¢‘ç‡: {pattern.frequency})")

    # æµ‹è¯•åŠ¨ä½œå»ºè®®
    should_avoid_look = memory.should_avoid_action("look")
    print(f"âœ… åº”è¯¥é¿å…'look'åŠ¨ä½œ: {should_avoid_look}")


def test_sequence_similarity():
    """æµ‹è¯•åºåˆ—ç›¸ä¼¼æ€§åˆ†æ"""
    print("\nğŸ”„ æµ‹è¯•åºåˆ—ç›¸ä¼¼æ€§åˆ†æ...")

    memory = MediumTermMemory("similarity_test")

    # æ·»åŠ ä¸€äº›å†å²åºåˆ—
    memory.start_new_sequence("test1")
    for action_name in ["take key", "go north", "open door"]:
        action = ActionMemory(action_name, "before", "after", 0.1, True, time.time())
        memory.add_action_to_sequence(action)
    memory.end_current_sequence(success=True)

    memory.start_new_sequence("test2")
    for action_name in ["take key", "go south", "open chest"]:
        action = ActionMemory(action_name, "before", "after", 0.2, True, time.time())
        memory.add_action_to_sequence(action)
    memory.end_current_sequence(success=True)

    # æµ‹è¯•ç›¸ä¼¼åºåˆ—æŸ¥æ‰¾
    current_actions = ["take key", "go north"]
    similar_sequences = memory.get_similar_sequences(current_actions)

    print(f"âœ… å½“å‰åŠ¨ä½œ: {current_actions}")
    print(f"âœ… æ‰¾åˆ°ç›¸ä¼¼åºåˆ—: {len(similar_sequences)}")
    for seq in similar_sequences:
        seq_actions = [action.action for action in seq.actions]
        print(f"  - {seq.sequence_id}: {seq_actions}")


def test_long_term_memory():
    """æµ‹è¯•é•¿æœŸè®°å¿†ç³»ç»Ÿ"""
    print("\nğŸ§  æµ‹è¯•é•¿æœŸè®°å¿†ç³»ç»Ÿ...")

    long_memory = LongTermMemory("test_long_term", max_patterns=20)

    # åˆ›å»ºä¸€äº›æµ‹è¯•åºåˆ—
    sequences = []

    # æˆåŠŸåºåˆ—1
    seq1 = ActionSequence(
        sequence_id="success_1",
        actions=[
            ActionMemory("take key", "see key", "have key", 0.5, True, time.time()),
            ActionMemory("go north", "have key", "in room", 0.1, True, time.time() + 1),
            ActionMemory("open door", "in room", "door opened", 1.0, True, time.time() + 2)
        ],
        start_time=time.time(),
        end_time=time.time() + 3,
        total_reward=1.6,
        success=True,
        goal="open door"
    )
    sequences.append(seq1)

    # æˆåŠŸåºåˆ—2 (ç›¸ä¼¼æ¨¡å¼)
    seq2 = ActionSequence(
        sequence_id="success_2",
        actions=[
            ActionMemory("take key", "see key", "have key", 0.5, True, time.time()),
            ActionMemory("go north", "have key", "in room", 0.1, True, time.time() + 1),
            ActionMemory("open door", "in room", "door opened", 1.0, True, time.time() + 2)
        ],
        start_time=time.time(),
        end_time=time.time() + 3,
        total_reward=1.6,
        success=True,
        goal="open door"
    )
    sequences.append(seq2)

    # å¤±è´¥åºåˆ—
    seq3 = ActionSequence(
        sequence_id="failure_1",
        actions=[
            ActionMemory("look", "in room", "see nothing", 0.0, False, time.time()),
            ActionMemory("look", "see nothing", "still nothing", -0.1, False, time.time() + 1)
        ],
        start_time=time.time(),
        end_time=time.time() + 2,
        total_reward=-0.1,
        success=False,
        goal="explore"
    )
    sequences.append(seq3)

    # è®©é•¿æœŸè®°å¿†å­¦ä¹ 
    long_memory.learn_from_sequences(sequences)

    # æ£€æŸ¥å­¦ä¹ ç»“æœ
    stats = long_memory.get_stats()
    print(f"âœ… é•¿æœŸè®°å¿†ç»Ÿè®¡: {stats}")

    # æ£€æŸ¥å†³ç­–æ¨¡å¼
    patterns = list(long_memory.decision_patterns.values())
    print(f"âœ… å­¦ä¹ åˆ°çš„å†³ç­–æ¨¡å¼: {len(patterns)}")
    for pattern in patterns:
        print(f"  - {pattern.pattern_type}: {pattern.description}")

    # æ£€æŸ¥ä»»åŠ¡ç­–ç•¥
    strategies = list(long_memory.task_strategies.values())
    print(f"âœ… å­¦ä¹ åˆ°çš„ä»»åŠ¡ç­–ç•¥: {len(strategies)}")
    for strategy in strategies:
        print(f"  - {strategy.task_type}: {len(strategy.strategy_steps)} æ­¥éª¤")

    # æ£€æŸ¥çŸ¥è¯†è§„åˆ™
    rules = long_memory.get_knowledge_rules()
    print(f"âœ… å­¦ä¹ åˆ°çš„çŸ¥è¯†è§„åˆ™: {len(rules)}")
    for rule in rules:
        print(f"  - {rule.rule_type}: {rule.condition} -> {rule.action}")

    return long_memory


def test_decision_patterns():
    """æµ‹è¯•å†³ç­–æ¨¡å¼æ¨è"""
    print("\nğŸ¯ æµ‹è¯•å†³ç­–æ¨¡å¼æ¨è...")

    long_memory = test_long_term_memory()

    # æµ‹è¯•å†³ç­–æ¨è
    context = {
        "goal": "open door",
        "available_actions_count": 3,
        "current_location": "kitchen"
    }

    recommendation = long_memory.get_decision_recommendation(context)

    if recommendation:
        print(f"âœ… æ¨èå†³ç­–æ¨¡å¼: {recommendation.description}")
        print(f"âœ… æ¨èåŠ¨ä½œåºåˆ—: {recommendation.actions}")
        print(f"âœ… ç½®ä¿¡åº¦: {recommendation.confidence:.2f}")
    else:
        print("âœ… æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å†³ç­–æ¨¡å¼")

    # æµ‹è¯•åŠ¨ä½œé¿å…å»ºè®®
    should_avoid_look = long_memory.should_avoid_action("look")
    print(f"âœ… åº”è¯¥é¿å…'look'åŠ¨ä½œ: {should_avoid_look}")

    # æµ‹è¯•åå¥½åŠ¨ä½œ
    preferred_actions = long_memory.get_preferred_actions()
    print(f"âœ… åå¥½åŠ¨ä½œ: {preferred_actions}")


def test_task_strategies():
    """æµ‹è¯•ä»»åŠ¡ç­–ç•¥"""
    print("\nğŸ“‹ æµ‹è¯•ä»»åŠ¡ç­–ç•¥...")

    long_memory = test_long_term_memory()

    # è·å–ä»»åŠ¡ç­–ç•¥
    strategy = long_memory.get_task_strategy("open door")

    if strategy:
        print(f"âœ… æ‰¾åˆ°ä»»åŠ¡ç­–ç•¥: {strategy.strategy_id}")
        print(f"âœ… ç­–ç•¥æœ‰æ•ˆæ€§: {strategy.effectiveness:.2f}")
        print(f"âœ… ç­–ç•¥é€‚åº”æ€§: {strategy.adaptability:.2f}")
        print(f"âœ… ç­–ç•¥æ­¥éª¤:")
        for step in strategy.strategy_steps:
            print(f"  {step['step_number']}. {step['action']}")
    else:
        print("âœ… æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡ç­–ç•¥")

    # æµ‹è¯•å…ƒå­¦ä¹ ç»Ÿè®¡
    meta_stats = long_memory.meta_statistics
    print(f"âœ… å…ƒå­¦ä¹ ç»Ÿè®¡:")
    print(f"  - æ€»ä¼šè¯æ•°: {meta_stats['total_sessions']}")
    print(f"  - æˆåŠŸä¼šè¯æ•°: {meta_stats['successful_sessions']}")
    print(f"  - å­¦ä¹ åˆ°çš„æ¨¡å¼æ•°: {meta_stats['total_patterns_learned']}")
    print(f"  - æŒ‰ç±»å‹åˆ†ç»„: {dict(meta_stats['patterns_by_type'])}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•è®°å¿†å¢å¼ºRAG Agent...\n")

    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_memory_enhanced_agent_creation()
        test_knowledge_integration()
        test_memory_guided_decision()
        test_loop_detection()
        test_memory_keyword_extraction()
        test_agent_stats()
        test_agent_reset()

        # æ–°å¢ä¸­æœŸè®°å¿†æµ‹è¯•
        test_medium_term_memory()
        test_pattern_analysis()
        test_sequence_similarity()

        # æ–°å¢é•¿æœŸè®°å¿†æµ‹è¯•
        test_long_term_memory()
        test_decision_patterns()
        test_task_strategies()

        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼è®°å¿†å¢å¼ºRAG Agentå’Œå®Œæ•´è®°å¿†ç³»ç»Ÿå·¥ä½œæ­£å¸¸ã€‚")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
