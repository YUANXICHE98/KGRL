## 基线对比表

| 论文 / 方法           | 环境 / 任务类型           | 基线模型 & 成绩                                               | RL 模型 & 成果                                                 | 提升幅度（绝对值）                            |
| --------------------- | ------------------------- | ------------------------------------------------------------- | -------------------------------------------------------------- | --------------------------------------------- |
| **GiGPO**       | ALFWorld / WebShop        | GRPO（参考值）                                                | GiGPO（两模型: 1.5B & 7B）                                     | ALFWorld: +12%；WebShop: +9% ([arXiv][1])     |
| **WebAgent-R1** | WebArena-Lite（Web 环境） | Prompting: Qwen-2.5-3B: 6.1%；Llama-3.1-8B: 8.5% ([arXiv][2]) | RL: Qwen-2.5-3B: 33.9%；Llama-3.1-8B: 44.8% ([arXiv][3])       | Qwen-2.5-3B: +27.8pts；Llama-3.1-8B: +36.3pts |
| **AgentPRM**    | ALFWorld（文本代理）      | GPT-4o ReAct baseline                                         | PRM 训练后的 3B 模型                                           | 直接超过 GPT-4o baseline（具体数值未给）      |
| **LOOP**        | AppWorld（代理 GUI 环境） | OpenAI o1 baseline                                            | LOOP PPO variant                                               | +9 pts 成功率 ([Hugging Face][4])             |
| **WebRL**       | WebArena-Lite             | Llama-3.1-8B: 4.8%；GLM-4-9B: 6.1% ([arXiv][5])               | RL 框架训练后：Llama-3.1-8B: 42.4%；GLM-4-9B: 43% ([arXiv][5]) | Llama-3.1-8B: +37.6 pts；GLM-4-9B: +36.9 pts  |

---

## 简要说明

* **GiGPO** 利用两层级的优势估计（episode + step level），提升显著。
* **WebAgent-R1** 在 Web 环境上将 3B 和 8B 模型的表现大幅提升到 33.9% 和 44.8%，起跳基线很低，改善幅度极大。
* **AgentPRM** 强调\*\*每步奖励（process reward）\*\*的作用，虽无具体数值，但直接超过了 GPT-4o ReAct 的表现。
* **LOOP** 提出了一个资源高效且工程上易部署的 PPO 变体，提升9点成功率。
* **WebRL** 采用在线 curriculum 教学策略，提升开源模型到显著高于 GPT-4o 的等级。

---

## 下一步建议

* **GiGPO** 和 **WebAgent-R1** 是最直观、最具竞争力的对标对象，可用来展现你的 RL 方法的优势。
* 如果你设计了 **过程奖励 + KG 约束机制**，可以参考 **AgentPRM** 的范式对比。
* 若你有资源优化需求，可模拟 **LOOP** 的架构来降低开销。
* 如果你希望使用开源模型作实验支撑，**WebRL** 提供了绝佳落地案例和性能目标。




## 1. 你期望的 KG 作用机制是什么？

你希望的是这样的流程：

* **KG 存储 DO–DA–F（Action / Condition / Outcome）结构化实体与关系** 。
* 模型通过** ****`query_kg(...)` 动态检索**来获得决策依据，而不是把知识直接堆进 prompt。
* 这样可以使 LLM 在决策时更有依据、更具可控性。

这是一种** ** **RAG 的结构化增强形式** ，既体现了实体关系、又可被运行时调用。

---

## 2. 当前有哪些类似工作的研究，它们实际做了什么？

下面列举几类相关研究，说明它们如何体现 KG 的动态作用，以及做到了怎样的效果：

### A.** ****Graph-RAG / Query-Driven Multimodal GraphRAG**

* 方法：在 RAG 中不是用一堆文本，而是用于生成的上下文来源是** ** **动态构建的本地知识子图（local knowledge subgraphs）** ，由用户查询决定结构如何展开。模型可以在 prompt 中看到结构化信息。([ResearchGate](https://www.researchgate.net/publication/389947823_KG-IRAG_A_Knowledge_Graph-Based_Iterative_Retrieval-Augmented_Generation_Framework_for_Temporal_Reasoning?utm_source=chatgpt.com "KG-IRAG: A Knowledge Graph-Based Iterative Retrieval ..."),** **[ACL Anthology](https://aclanthology.org/2025.findings-acl.1100.pdf?utm_source=chatgpt.com "Query-Driven Multimodal GraphRAG: Dynamic Local ..."))
* 成果：在 MultimodalQA 和 WebQA 等任务上，作为无监督方法，取得了当前最优表现。([ACL Anthology](https://aclanthology.org/2025.findings-acl.1100.pdf?utm_source=chatgpt.com "Query-Driven Multimodal GraphRAG: Dynamic Local ..."))

### B.** ****DyG-RAG（Dynamic Graph RAG）**

* 方法：专注于时态和事件推理。将文本转成**动态事件单元（DEUs）**作为 KG 节点，构建事件图；并通过“时间链式思维（Time CoT）”引导模型跟随时间顺序推理。 retrieval 是基于时间和事件图结构的“事件时间线检索”。([arXiv](https://arxiv.org/html/2507.13396v1?utm_source=chatgpt.com "DyG-RAG: Dynamic Graph Retrieval-Augmented ..."))
* 成果：在时间事实问答上显著提升准确率和召回率，比标准 RAG 强得多。

### C.** ****DO-RAG**

* 方法：一个专门设计用于领域 QA 的架构。从非结构化文档中自动构建多层次 KG，然后在检索时结合图遍历（KG）与向量查找（semantic retrieval），最后通过生成后校正来减少幻觉。([arXiv](https://arxiv.org/html/2505.17058v1?utm_source=chatgpt.com "DO-RAG: A Domain-Specific QA Framework Using ..."))
* 成果：在数据库、电力领域的测试上，召回率接近完美，回答相关性超过 94%，超越现有 RAG 方法最多 33.4%。

### D.** ****OG-RAG（Ontology-Grounded RAG）**

* 方法：KG 源于领域本体（ontology），检索过程基于超图（hypergraph）优化构造上下文，保证检索出的内容结构准确、少冗余。([arXiv](https://arxiv.org/abs/2412.15235?utm_source=chatgpt.com "OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large Language Models"))
* 成果：提高准确的事实召回率 55%，响应正确性提升 40%，属性推理准确率提升 27%。

---

## 3. 成果总结对比

| 方法                              | 动态调用 KG (而非静态拼 prompt)  | 特点与创新                              | 实验结果亮点                                                                                                                                                                                                                                                                                                                    |
| --------------------------------- | -------------------------------- | --------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| GraphRAG / Query‑Driven GraphRAG | 有，构建子图后检索用入 prompt    | 动态本地图路径检索，结构化增强检索      | MultimodalQA/WebQA 上无监督 SOTA ([arXiv](https://arxiv.org/abs/2412.15235?utm_source=chatgpt.com "OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large Language Models"), [ACL Anthology](https://aclanthology.org/2025.findings-acl.1100.pdf?utm_source=chatgpt.com "Query-Driven Multimodal GraphRAG: Dynamic Local ...")) |
| DyG-RAG                           | 有，事件图 + 时间链检索          | 显式模型时间事件结构，Time-CoT 时间推理 | 时间型问答显著提升准确率召回率 ([arXiv](https://arxiv.org/html/2507.13396v1?utm_source=chatgpt.com "DyG-RAG: Dynamic Graph Retrieval-Augmented ..."))                                                                                                                                                                                 |
| DO-RAG                            | 有，动态图 + 向量检索融合        | KG 自动构建、多级融合、后验纠正幻觉     | 专业领域 QA 上召回近 100%，相关性 >94% ([arXiv](https://arxiv.org/html/2505.17058v1?utm_source=chatgpt.com "DO-RAG: A Domain-Specific QA Framework Using ..."))                                                                                                                                                                       |
| OG-RAG                            | 有，ontology-grounded 图检索优化 | 本体驱动检索、超图上下文最小化优化      | 减少错误率、推理准确率提升显著 ([arXiv](https://arxiv.org/abs/2412.15235?utm_source=chatgpt.com "OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large Language Models"))                                                                                                                                                |

---

## 4. 小结

 **当前已有的工作确实支持“KG 动态调用”的思路** ，并不只是静态背景注入。方法上多通过图结构（事件图、本体图、多层 KG）加上检索机制来使 LLM 在决策时显式使用KG。

* 在时空或 DO–DA–F 场景下，类似** ****DyG-RAG** 的方法已经做过动态事件图 + CoT 推理；
* **DO-RAG** 则展示了 KG 自动构建 + RAG 在专业领域问答中的可靠性；
* **OG-RAG** 提供了本体驱动的结构检索方式，提升整体结构一致性与准确率。

你可以参考这些方法，将你的 DO–DA–F KG 用在** ****ReAct + query_kg** 环境下，实现动作决策前的条件查询与反馈检索机制，从而真正让 KG 发挥结构化、动态作用。

如果需要，我还可以帮你做一个该领域的调用示意代码，或者基于 DyG-RAG/DO-RAG 框架搭一个 mini-version DEMO，随时告诉我！


[1]: https://arxiv.org/abs/2505.10978?utm_source=chatgpt.com
[2]: https://arxiv.org/abs/2505.16421?utm_source=chatgpt.com
[3]: https://arxiv.org/html/2505.16421v1?utm_source=chatgpt.com
[4]: https://huggingface.co/papers?q=RL+performance+optimization&utm_source=chatgpt.com
[5]: https://arxiv.org/abs/2411.02337?utm_source=chatgpt.com
